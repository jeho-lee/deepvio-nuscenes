{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.can_bus.can_bus_api import NuScenesCanBus\n",
    "import numpy as np\n",
    "import pprint\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from path import Path\n",
    "from utils import custom_transform\n",
    "from dataset.KITTI_dataset import KITTI\n",
    "from model import DeepVIO\n",
    "from collections import defaultdict\n",
    "from utils.kitti_eval import KITTI_tester, data_partition\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from utils.utils import *\n",
    "\n",
    "from utils.utils import rotationError, read_pose_from_text\n",
    "from collections import Counter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "from scipy.ndimage import convolve1d\n",
    "from torch.utils.data import Dataset\n",
    "from utils import custom_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_rotation_matrix(Q):\n",
    "    \"\"\"\n",
    "    Covert a quaternion into a full three-dimensional rotation matrix.\n",
    " \n",
    "    Input\n",
    "    :param Q: A 4 element array representing the quaternion (q0,q1,q2,q3) \n",
    " \n",
    "    Output\n",
    "    :return: A 3x3 element matrix representing the full 3D rotation matrix. \n",
    "             This rotation matrix converts a point in the local reference \n",
    "             frame to a point in the global reference frame.\n",
    "    \"\"\"\n",
    "    # Extract the values from Q\n",
    "    q0 = Q[0]\n",
    "    q1 = Q[1]\n",
    "    q2 = Q[2]\n",
    "    q3 = Q[3]\n",
    "     \n",
    "    # First row of the rotation matrix\n",
    "    r00 = 2 * (q0 * q0 + q1 * q1) - 1\n",
    "    r01 = 2 * (q1 * q2 - q0 * q3)\n",
    "    r02 = 2 * (q1 * q3 + q0 * q2)\n",
    "     \n",
    "    # Second row of the rotation matrix\n",
    "    r10 = 2 * (q1 * q2 + q0 * q3)\n",
    "    r11 = 2 * (q0 * q0 + q2 * q2) - 1\n",
    "    r12 = 2 * (q2 * q3 - q0 * q1)\n",
    "     \n",
    "    # Third row of the rotation matrix\n",
    "    r20 = 2 * (q1 * q3 - q0 * q2)\n",
    "    r21 = 2 * (q2 * q3 + q0 * q1)\n",
    "    r22 = 2 * (q0 * q0 + q3 * q3) - 1\n",
    "     \n",
    "    # 3x3 rotation matrix\n",
    "    rot_matrix = np.array([[r00, r01, r02],\n",
    "                           [r10, r11, r12],\n",
    "                           [r20, r21, r22]])\n",
    "                            \n",
    "    return rot_matrix\n",
    "\n",
    "_EPS = np.finfo(float).eps * 4.0\n",
    "def euler_from_matrix(matrix):\n",
    "    '''\n",
    "    Extract the eular angle from a rotation matrix\n",
    "    '''\n",
    "    M = np.array(matrix, dtype=np.float64, copy=False)[:3, :3]\n",
    "    cy = math.sqrt(M[0, 0] * M[0, 0] + M[1, 0] * M[1, 0])\n",
    "    ay = math.atan2(-M[2, 0], cy)\n",
    "    if ay < -math.pi / 2 + _EPS and ay > -math.pi / 2 - _EPS:  # pitch = -90 deg\n",
    "        ax = 0\n",
    "        az = math.atan2(-M[1, 2], -M[0, 2])\n",
    "    elif ay < math.pi / 2 + _EPS and ay > math.pi / 2 - _EPS:\n",
    "        ax = 0\n",
    "        az = math.atan2(M[1, 2], M[0, 2])\n",
    "    else:\n",
    "        ax = math.atan2(M[2, 1], M[2, 2])\n",
    "        az = math.atan2(M[1, 0], M[0, 0])\n",
    "    return np.array([ax, ay, az])\n",
    "\n",
    "def get_lds_kernel_window(kernel, ks, sigma):\n",
    "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "    half_ks = (ks - 1) // 2\n",
    "    if kernel == 'gaussian':\n",
    "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "    elif kernel == 'triang':\n",
    "        kernel_window = triang(ks)\n",
    "    else:\n",
    "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "    return kernel_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = '/data/public/360_3D_OD_Dataset/nuscenes'\n",
    "nusc_can = NuScenesCanBus(dataroot=dataroot)\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot=dataroot, verbose=False)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "max_imu_length = 10\n",
    "##############################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# TODO 100hz imu, 10hz image and pose vs. 96 hz imu, 12 hz image and pose\n",
    "# 이미지 간 격차가 큰거보다 imu 간 격차가 커지는게 더 나은게, IMU는 애초에 hz가 높기 떄문에 몇개 없어지더라도 큰 영향이 없음\n",
    "# ==> 12hz image, pose 쓰고, imu는 96hz로 downsampling\n",
    "##############################################################\n",
    "\n",
    "# temp: get first scene\n",
    "scene = nusc.scene[0]\n",
    "scene_name = scene['name']\n",
    "scene_token = scene['token']\n",
    "\n",
    "# Get images and poses of target scene\n",
    "first_sample_token = scene['first_sample_token']\n",
    "cur_sample = nusc.get('sample', first_sample_token)\n",
    "cur_sample_data = nusc.get('sample_data', cur_sample['data']['CAM_FRONT']) # TODO\n",
    "\n",
    "scene_sample_data = []\n",
    "while True:\n",
    "    try:\n",
    "        scene_sample_data.append(cur_sample_data)\n",
    "        cur_sample_data = nusc.get('sample_data', cur_sample_data['next'])\n",
    "    except:\n",
    "        break\n",
    "\n",
    "scene_imu_data = nusc_can.get_messages(scene_name, 'ms_imu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Collect image (12hz), pose (12hz), imu data (96hz) of target scene - single training input contains 2 images,  \"\"\"\n",
    "\n",
    "# 1. 일단 각 training input 모으기 - 2 images, 2 pose, 1 relative pose, 8 imu data\n",
    "\n",
    "# spread imu data evenly across image/pose data\n",
    "\n",
    "training_inputs = []\n",
    "\n",
    "for data_idx, cur_sample_data in enumerate(scene_sample_data):\n",
    "    \n",
    "    # 1. get image \n",
    "    cur_img_path = os.path.join(dataroot, cur_sample_data['filename'])\n",
    "    if cur_sample_data['next'] != \"\":\n",
    "        next_sample_data = nusc.get('sample_data', cur_sample_data['next'])\n",
    "        next_img_path = os.path.join(dataroot, next_sample_data['filename'])\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    # 2. get ego pose\n",
    "    # read_pose in utils.py\n",
    "    cur_ego_pose = nusc.get('ego_pose', cur_sample_data['ego_pose_token'])\n",
    "    trans = np.array(cur_ego_pose['translation'])\n",
    "    trans = trans.reshape(3, -1)\n",
    "    rot_mat = quaternion_rotation_matrix(cur_ego_pose['rotation']) # (w, x, y, z)\n",
    "    cur_ego_pose_mat = np.concatenate((rot_mat, trans), axis=1)\n",
    "    cur_ego_pose_mat = np.array(cur_ego_pose_mat).reshape(3, 4)\n",
    "    cur_ego_pose_mat = np.concatenate((cur_ego_pose_mat, np.array([[0, 0, 0, 1]])), 0)\n",
    "    \n",
    "    next_ego_pose = nusc.get('ego_pose', next_sample_data['ego_pose_token'])\n",
    "    trans = np.array(next_ego_pose['translation'])\n",
    "    trans = trans.reshape(3, -1)\n",
    "    rot_mat = quaternion_rotation_matrix(next_ego_pose['rotation']) # (w, x, y, z)\n",
    "    next_ego_pose_mat = np.concatenate((rot_mat, trans), axis=1)\n",
    "    next_ego_pose_mat = np.array(next_ego_pose_mat).reshape(3, 4)\n",
    "    next_ego_pose_mat = np.concatenate((next_ego_pose_mat, np.array([[0, 0, 0, 1]])), 0)    \n",
    "\n",
    "    # 3. get relative pose\n",
    "    relative_pose = np.dot(np.linalg.inv(cur_ego_pose_mat), next_ego_pose_mat)\n",
    "    R_rel = relative_pose[:3, :3]\n",
    "    t_rel = relative_pose[:3, 3]\n",
    "\n",
    "        # Extract the Eular angle from the relative rotation matrix\n",
    "    x, y, z = euler_from_matrix(R_rel)\n",
    "    theta = [x, y, z]\n",
    "\n",
    "    pose_rel = np.concatenate((theta, t_rel))\n",
    "    \n",
    "    # 4. get imu data\n",
    "    # next_timestamp = next_sample_data['timestamp'] / 1e6\n",
    "    cur_timestamp = cur_sample_data['timestamp'] / 1e6\n",
    "    next_timestamp = next_sample_data['timestamp'] / 1e6\n",
    "    \n",
    "    # get imu data between cur and next timestamp\n",
    "    imu_data = []\n",
    "    for imu in scene_imu_data:\n",
    "        imu_timestamp = imu['utime'] / 1e6\n",
    "        if imu_timestamp > cur_timestamp and imu_timestamp < next_timestamp:\n",
    "            data = imu['linear_accel'] + imu['rotation_rate']\n",
    "            imu_data.append(data)\n",
    "    \n",
    "    # if imu data length is less than max_imu_length, pad with zeros\n",
    "    if len(imu_data) < max_imu_length:\n",
    "        imu_data = np.pad(imu_data, ((0, max_imu_length - len(imu_data)), (0, 0)), 'constant', constant_values=0)\n",
    "    else:\n",
    "        imu_data = imu_data[:max_imu_length]\n",
    "    \n",
    "    # 5. make training input\n",
    "    training_input = {\n",
    "        'cur_img_path': cur_img_path,\n",
    "        'next_img_path': next_img_path,\n",
    "        'cur_ego_pose': cur_ego_pose_mat,\n",
    "        'next_ego_pose': next_ego_pose_mat,\n",
    "        'pose_rel': pose_rel,\n",
    "        'imu_data': imu_data\n",
    "    }\n",
    "    training_inputs.append(training_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 11\n",
    "\n",
    "samples = []\n",
    "\n",
    "input_idx = 0\n",
    "while True:\n",
    "    # get training input chunk of sequence_length\n",
    "    training_input_chunk = training_inputs[input_idx : input_idx + (sequence_length-1)]\n",
    "    input_idx += 1 # training sequence간 겹치는 images 존재함\n",
    "    if len(training_input_chunk) < (sequence_length-1):\n",
    "        print(len(training_input_chunk))\n",
    "        break\n",
    "    \n",
    "    img_samples = []\n",
    "    pose_samples = []\n",
    "    for training_input in training_input_chunk:\n",
    "        img_samples.append(training_input['cur_img_path'])\n",
    "        pose_samples.append(training_input['cur_ego_pose'])\n",
    "    img_samples.append(training_input_chunk[-1]['next_img_path'])\n",
    "    pose_samples.append(training_input_chunk[-1]['next_ego_pose'])\n",
    "    \n",
    "    pose_rel_samples = []\n",
    "    imu_samples = []\n",
    "    for training_input in training_input_chunk:\n",
    "        pose_rel_samples.append(training_input['pose_rel'])\n",
    "        imu_samples.append(np.array(training_input['imu_data']))\n",
    "    \n",
    "    pose_samples = np.array(pose_samples)\n",
    "    pose_rel_samples = np.array(pose_rel_samples)\n",
    "    # imu_samples = np.array(imu_samples, dtype=np.float32)\n",
    "    imu_samples = np.array(imu_samples)\n",
    "    \n",
    "    segment_rot = rotationError(pose_samples[0], pose_samples[-1])\n",
    "    sample = {'imgs':img_samples, 'imus':imu_samples, 'gts': pose_rel_samples, 'rot': segment_rot}\n",
    "    \n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate weights based on the rotation of the training segments\n",
    "# Weights are calculated based on the histogram of rotations according to the method in https://github.com/YyzHarry/imbalanced-regression\n",
    "rot_list = np.array([np.cbrt(item['rot']*180/np.pi) for item in samples])\n",
    "rot_range = np.linspace(np.min(rot_list), np.max(rot_list), num=10)\n",
    "indexes = np.digitize(rot_list, rot_range, right=False)\n",
    "num_samples_of_bins = dict(Counter(indexes))\n",
    "emp_label_dist = [num_samples_of_bins.get(i, 0) for i in range(1, len(rot_range)+1)]\n",
    "\n",
    "# Apply 1d convolution to get the smoothed effective label distribution\n",
    "lds_kernel_window = get_lds_kernel_window(kernel='gaussian', ks=7, sigma=5)\n",
    "eff_label_dist = convolve1d(np.array(emp_label_dist), weights=lds_kernel_window, mode='constant')\n",
    "\n",
    "weights = [np.float32(1/eff_label_dist[bin_idx-1]) for bin_idx in indexes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nusc_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_root,\n",
    "                 sequence_length=11,\n",
    "                 max_imu_length=10,\n",
    "                #  train_seqs=['00', '01', '02', '04', '06', '08', '09'],\n",
    "                 transform=None,\n",
    "                 nusc=None,\n",
    "                 nusc_can=None):\n",
    "        \n",
    "        self.data_root = data_root\n",
    "        if nusc is None:\n",
    "            self.nusc = NuScenes(version='v1.0-trainval', dataroot=self.data_root, verbose=False)\n",
    "        else:\n",
    "            self.nusc = nusc\n",
    "        if nusc_can is None:\n",
    "            self.nusc_can = NuScenesCanBus(dataroot=self.data_root)\n",
    "        else:\n",
    "            self.nusc_can = nusc_can\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_imu_length = max_imu_length\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.make_dataset()\n",
    "    \n",
    "    def get_scene_data(self, scene_idx):\n",
    "        scene = nusc.scene[scene_idx]\n",
    "        scene_name = scene['name']\n",
    "        scene_token = scene['token']\n",
    "\n",
    "        # Get images and poses of target scene\n",
    "        first_sample_token = scene['first_sample_token']\n",
    "        cur_sample = nusc.get('sample', first_sample_token)\n",
    "        cur_sample_data = nusc.get('sample_data', cur_sample['data']['CAM_FRONT']) # TODO\n",
    "\n",
    "        scene_sample_data = []\n",
    "        while True:\n",
    "            try:\n",
    "                scene_sample_data.append(cur_sample_data)\n",
    "                cur_sample_data = nusc.get('sample_data', cur_sample_data['next'])\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        scene_imu_data = nusc_can.get_messages(scene_name, 'ms_imu')\n",
    "        return scene_sample_data, scene_imu_data\n",
    "    \n",
    "    def format_training_inputs(self, scene_sample_data, scene_imu_data):\n",
    "        \"\"\" Collect image (12hz), pose (12hz), imu data (96hz) of target scene - single training input contains 2 images,  \"\"\"\n",
    "        # 1. 일단 각 training input 모으기 - 2 images, 2 pose, 1 relative pose, 8 imu data\n",
    "        training_inputs = []\n",
    "        for data_idx, cur_sample_data in enumerate(scene_sample_data):\n",
    "            \n",
    "            # 1. get image \n",
    "            cur_img_path = os.path.join(dataroot, cur_sample_data['filename'])\n",
    "            if cur_sample_data['next'] != \"\":\n",
    "                next_sample_data = nusc.get('sample_data', cur_sample_data['next'])\n",
    "                next_img_path = os.path.join(dataroot, next_sample_data['filename'])\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "            # 2. get ego pose\n",
    "            # read_pose in utils.py\n",
    "            cur_ego_pose = nusc.get('ego_pose', cur_sample_data['ego_pose_token'])\n",
    "            trans = np.array(cur_ego_pose['translation'])\n",
    "            trans = trans.reshape(3, -1)\n",
    "            rot_mat = quaternion_rotation_matrix(cur_ego_pose['rotation']) # (w, x, y, z)\n",
    "            cur_ego_pose_mat = np.concatenate((rot_mat, trans), axis=1)\n",
    "            cur_ego_pose_mat = np.array(cur_ego_pose_mat).reshape(3, 4)\n",
    "            cur_ego_pose_mat = np.concatenate((cur_ego_pose_mat, np.array([[0, 0, 0, 1]])), 0)\n",
    "            \n",
    "            next_ego_pose = nusc.get('ego_pose', next_sample_data['ego_pose_token'])\n",
    "            trans = np.array(next_ego_pose['translation'])\n",
    "            trans = trans.reshape(3, -1)\n",
    "            rot_mat = quaternion_rotation_matrix(next_ego_pose['rotation']) # (w, x, y, z)\n",
    "            next_ego_pose_mat = np.concatenate((rot_mat, trans), axis=1)\n",
    "            next_ego_pose_mat = np.array(next_ego_pose_mat).reshape(3, 4)\n",
    "            next_ego_pose_mat = np.concatenate((next_ego_pose_mat, np.array([[0, 0, 0, 1]])), 0)    \n",
    "\n",
    "            # 3. get relative pose\n",
    "            relative_pose = np.dot(np.linalg.inv(cur_ego_pose_mat), next_ego_pose_mat)\n",
    "            R_rel = relative_pose[:3, :3]\n",
    "            t_rel = relative_pose[:3, 3]\n",
    "\n",
    "                # Extract the Eular angle from the relative rotation matrix\n",
    "            x, y, z = euler_from_matrix(R_rel)\n",
    "            theta = [x, y, z]\n",
    "\n",
    "            pose_rel = np.concatenate((theta, t_rel))\n",
    "            \n",
    "            # 4. get imu data\n",
    "            # next_timestamp = next_sample_data['timestamp'] / 1e6\n",
    "            cur_timestamp = cur_sample_data['timestamp'] / 1e6\n",
    "            next_timestamp = next_sample_data['timestamp'] / 1e6\n",
    "            \n",
    "            # get imu data between cur and next timestamp\n",
    "            imu_data = []\n",
    "            for imu in scene_imu_data:\n",
    "                imu_timestamp = imu['utime'] / 1e6\n",
    "                if imu_timestamp > cur_timestamp and imu_timestamp < next_timestamp:\n",
    "                    data = imu['linear_accel'] + imu['rotation_rate']\n",
    "                    imu_data.append(data)\n",
    "            \n",
    "            # if imu data length is less than max_imu_length, pad with zeros\n",
    "            if len(imu_data) < self.max_imu_length:\n",
    "                imu_data = np.pad(imu_data, ((0, self.max_imu_length - len(imu_data)), (0, 0)), 'constant', constant_values=0)\n",
    "            else:\n",
    "                imu_data = imu_data[:self.max_imu_length]\n",
    "            \n",
    "            # 5. make training input\n",
    "            training_input = {\n",
    "                'cur_img_path': cur_img_path,\n",
    "                'next_img_path': next_img_path,\n",
    "                'cur_ego_pose': cur_ego_pose_mat,\n",
    "                'next_ego_pose': next_ego_pose_mat,\n",
    "                'pose_rel': pose_rel,\n",
    "                'imu_data': imu_data\n",
    "            }\n",
    "            training_inputs.append(training_input)\n",
    "        return training_inputs\n",
    "    \n",
    "    def segment_training_inputs(self, training_inputs):\n",
    "        samples = []\n",
    "\n",
    "        input_idx = 0\n",
    "        while True:\n",
    "            # get training input chunk of sequence_length\n",
    "            training_input_chunk = training_inputs[input_idx : input_idx + (self.sequence_length-1)]\n",
    "            \n",
    "            # input_idx = input_idx + (sequence_length-1)\n",
    "            input_idx += 1 # training sequence간 겹치는 images 존재함\n",
    "            \n",
    "            if len(training_input_chunk) < (self.sequence_length-1):\n",
    "                print(len(training_input_chunk))\n",
    "                break\n",
    "            \n",
    "            img_samples = []\n",
    "            pose_samples = []\n",
    "            for training_input in training_input_chunk:\n",
    "                img_samples.append(training_input['cur_img_path'])\n",
    "                pose_samples.append(training_input['cur_ego_pose'])\n",
    "            img_samples.append(training_input_chunk[-1]['next_img_path'])\n",
    "            pose_samples.append(training_input_chunk[-1]['next_ego_pose'])\n",
    "            \n",
    "            pose_rel_samples = []\n",
    "            # imu_samples = []\n",
    "            imu_samples = np.empty((0, 6))\n",
    "            for training_input in training_input_chunk:\n",
    "                pose_rel_samples.append(training_input['pose_rel'])\n",
    "                # imu_samples.append(np.array(training_input['imu_data']))\n",
    "                imu_samples = np.vstack((imu_samples, np.array(training_input['imu_data'])))\n",
    "            \n",
    "            pose_samples = np.array(pose_samples)\n",
    "            pose_rel_samples = np.array(pose_rel_samples)\n",
    "            imu_samples = np.array(imu_samples)\n",
    "    \n",
    "            segment_rot = rotationError(pose_samples[0], pose_samples[-1])\n",
    "            sample = {'imgs':img_samples, 'imus':imu_samples, 'gts': pose_rel_samples, 'rot': segment_rot}\n",
    "            \n",
    "            samples.append(sample)\n",
    "            \n",
    "        # Generate weights based on the rotation of the training segments\n",
    "        # Weights are calculated based on the histogram of rotations according to the method in https://github.com/YyzHarry/imbalanced-regression\n",
    "        rot_list = np.array([np.cbrt(item['rot']*180/np.pi) for item in samples])\n",
    "        rot_range = np.linspace(np.min(rot_list), np.max(rot_list), num=10)\n",
    "        indexes = np.digitize(rot_list, rot_range, right=False)\n",
    "        num_samples_of_bins = dict(Counter(indexes))\n",
    "        emp_label_dist = [num_samples_of_bins.get(i, 0) for i in range(1, len(rot_range)+1)]\n",
    "\n",
    "        # Apply 1d convolution to get the smoothed effective label distribution\n",
    "        lds_kernel_window = get_lds_kernel_window(kernel='gaussian', ks=7, sigma=5)\n",
    "        eff_label_dist = convolve1d(np.array(emp_label_dist), weights=lds_kernel_window, mode='constant')\n",
    "\n",
    "        weights = [np.float32(1/eff_label_dist[bin_idx-1]) for bin_idx in indexes]\n",
    "        \n",
    "        return samples, weights\n",
    "    \n",
    "    def make_dataset(self):\n",
    "        \n",
    "        # temp: get first scene\n",
    "        scene_sample_data, scene_imu_data = self.get_scene_data(0)\n",
    "        \n",
    "        scene_training_inputs = self.format_training_inputs(scene_sample_data, scene_imu_data)\n",
    "        \n",
    "        self.samples, self.weights = self.segment_training_inputs(scene_training_inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        imgs = [np.asarray(Image.open(img)) for img in sample['imgs']]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            # imgs, imus, gts = self.transform(imgs, np.copy(sample['imus']), np.copy(sample['gts']))\n",
    "            imgs, imus, gts = self.transform(imgs, np.copy(sample['imus']).astype(np.float32), np.copy(sample['gts']).astype(np.float32))\n",
    "        else:\n",
    "            imus = np.copy(sample['imus'])\n",
    "            gts = np.copy(sample['gts']).astype(np.float32)\n",
    "        \n",
    "        rot = sample['rot'].astype(np.float32)\n",
    "        weight = self.weights[index]\n",
    "\n",
    "        return imgs, imus, gts, rot, weight\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "dataroot = '/data/public/360_3D_OD_Dataset/nuscenes'\n",
    "device = 'cuda:1'\n",
    "#########################################################################################\n",
    "\n",
    "nusc_can = NuScenesCanBus(dataroot=dataroot)\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot=dataroot, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--data_dir', type=str, default='/nfs/turbo/coe-hunseok/mingyuy/KITTI_odometry', help='path to the dataset')\n",
    "parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
    "parser.add_argument('--save_dir', type=str, default='./results', help='path to save the result')\n",
    "\n",
    "parser.add_argument('--train_seq', nargs='+', default=['00', '01', '02', '04'], help='sequences for training')\n",
    "parser.add_argument('--val_seq', nargs='+', default=['03'], help='sequences for validation')\n",
    "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
    "\n",
    "parser.add_argument('--img_w', type=int, default=512, help='image width')\n",
    "parser.add_argument('--img_h', type=int, default=256, help='image height')\n",
    "parser.add_argument('--v_f_len', type=int, default=512, help='visual feature length')\n",
    "parser.add_argument('--i_f_len', type=int, default=256, help='imu feature length')\n",
    "parser.add_argument('--fuse_method', type=str, default='cat', help='fusion method [cat, soft, hard]')\n",
    "parser.add_argument('--imu_dropout', type=float, default=0, help='dropout for the IMU encoder')\n",
    "\n",
    "parser.add_argument('--rnn_hidden_size', type=int, default=1024, help='size of the LSTM latent')\n",
    "parser.add_argument('--rnn_dropout_out', type=float, default=0.2, help='dropout for the LSTM output layer')\n",
    "parser.add_argument('--rnn_dropout_between', type=float, default=0.2, help='dropout within LSTM')\n",
    "\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-6, help='weight decay for the optimizer')\n",
    "parser.add_argument('--batch_size', type=int, default=8, help='batch size')\n",
    "parser.add_argument('--seq_len', type=int, default=11, help='sequence length for LSTM')\n",
    "parser.add_argument('--workers', type=int, default=4, help='number of workers')\n",
    "parser.add_argument('--epochs_warmup', type=int, default=40, help='number of epochs for warmup')\n",
    "parser.add_argument('--epochs_joint', type=int, default=40, help='number of epochs for joint training')\n",
    "parser.add_argument('--epochs_fine', type=int, default=20, help='number of epochs for finetuning')\n",
    "parser.add_argument('--lr_warmup', type=float, default=5e-4, help='learning rate for warming up stage')\n",
    "parser.add_argument('--lr_joint', type=float, default=5e-5, help='learning rate for joint training stage')\n",
    "parser.add_argument('--lr_fine', type=float, default=1e-6, help='learning rate for finetuning stage')\n",
    "parser.add_argument('--eta', type=float, default=0.05, help='exponential decay factor for temperature')\n",
    "parser.add_argument('--temp_init', type=float, default=5, help='initial temperature for gumbel-softmax')\n",
    "parser.add_argument('--Lambda', type=float, default=3e-5, help='penalty factor for the visual encoder usage')\n",
    "\n",
    "parser.add_argument('--experiment_name', type=str, default='experiment', help='experiment name')\n",
    "parser.add_argument('--optimizer', type=str, default='Adam', help='type of optimizer [Adam, SGD]')\n",
    "\n",
    "parser.add_argument('--pretrain_flownet',type=str, default='./pretrained_models/flownets_bn_EPE2.459.pth.tar', help='wehther to use the pre-trained flownet')\n",
    "parser.add_argument('--pretrain', type=str, default=None, help='path to the pretrained model')\n",
    "parser.add_argument('--hflip', default=False, action='store_true', help='whether to use horizonal flipping as augmentation')\n",
    "parser.add_argument('--color', default=False, action='store_true', help='whether to use color augmentations')\n",
    "\n",
    "parser.add_argument('--print_frequency', type=int, default=10, help='print frequency for loss values')\n",
    "parser.add_argument('--weighted', default=False, action='store_true', help='whether to use weighted sum')\n",
    "\n",
    "parser.add_argument('--mode', type=str, default='kitti', help='types of dataset [kitti, kitti_5hz, campus, nuscenes]')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "def update_status(ep, args, model):\n",
    "    if ep < args.epochs_warmup:  # Warmup stage\n",
    "        lr = args.lr_warmup\n",
    "        selection = 'random'\n",
    "        temp = args.temp_init\n",
    "        for param in model.module.Policy_net.parameters(): # Disable the policy network\n",
    "            param.requires_grad = False\n",
    "    elif ep >= args.epochs_warmup and ep < args.epochs_warmup + args.epochs_joint: # Joint training stage\n",
    "        lr = args.lr_joint\n",
    "        selection = 'gumbel-softmax'\n",
    "        temp = args.temp_init * math.exp(-args.eta * (ep-args.epochs_warmup))\n",
    "        for param in model.module.Policy_net.parameters(): # Enable the policy network\n",
    "            param.requires_grad = True\n",
    "    elif ep >= args.epochs_warmup + args.epochs_joint: # Finetuning stage\n",
    "        lr = args.lr_fine\n",
    "        selection = 'gumbel-softmax'\n",
    "        temp = args.temp_init * math.exp(-args.eta * (ep-args.epochs_warmup))\n",
    "    return lr, selection, temp\n",
    "\n",
    "def train(model, optimizer, train_loader, selection, temp, logger, ep, p=0.5, weighted=False):\n",
    "    \n",
    "    mse_losses = []\n",
    "    penalties = []\n",
    "    data_len = len(train_loader)\n",
    "\n",
    "    for i, (imgs, imus, gts, rot, weight) in enumerate(train_loader):\n",
    "\n",
    "        imgs = imgs.cuda().float()\n",
    "        imus = imus.cuda().float()\n",
    "        gts = gts.cuda().float() \n",
    "        weight = weight.cuda().float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        poses, decisions, probs, _ = model(imgs, imus, is_first=True, hc=None, temp=temp, selection=selection, p=p)\n",
    "        \n",
    "        if not weighted:\n",
    "            angle_loss = torch.nn.functional.mse_loss(poses[:,:,:3], gts[:, :, :3])\n",
    "            translation_loss = torch.nn.functional.mse_loss(poses[:,:,3:], gts[:, :, 3:])\n",
    "        else:\n",
    "            weight = weight/weight.sum()\n",
    "            angle_loss = (weight.unsqueeze(-1).unsqueeze(-1) * (poses[:,:,:3] - gts[:, :, :3]) ** 2).mean()\n",
    "            translation_loss = (weight.unsqueeze(-1).unsqueeze(-1) * (poses[:,:,3:] - gts[:, :, 3:]) ** 2).mean()\n",
    "        \n",
    "        pose_loss = 100 * angle_loss + translation_loss        \n",
    "        penalty = (decisions[:,:,0].float()).sum(-1).mean()\n",
    "        loss = pose_loss + args.Lambda * penalty \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % args.print_frequency == 0: \n",
    "            message = f'Epoch: {ep}, iters: {i}/{data_len}, pose loss: {pose_loss.item():.6f}, penalty: {penalty.item():.6f}, loss: {loss.item():.6f}'\n",
    "            print(message)\n",
    "            logger.info(message)\n",
    "\n",
    "        mse_losses.append(pose_loss.item())\n",
    "        penalties.append(penalty.item())\n",
    "\n",
    "    return np.mean(mse_losses), np.mean(penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Training from scratch\n",
      "Epoch: 0, lr: 0.0005, selection: random, temperaure: 5.00000\n",
      "Epoch: 0, iters: 0/28, pose loss: 9.580170, penalty: 4.500000, loss: 9.580305\n",
      "Epoch: 0, iters: 10/28, pose loss: 1.520292, penalty: 4.625000, loss: 1.520431\n",
      "Epoch: 0, iters: 20/28, pose loss: 0.423523, penalty: 4.375000, loss: 0.423655\n",
      "Epoch: 1, lr: 0.0005, selection: random, temperaure: 5.00000\n",
      "Epoch: 1, iters: 0/28, pose loss: 0.285116, penalty: 5.375000, loss: 0.285277\n",
      "Epoch: 1, iters: 10/28, pose loss: 0.213534, penalty: 4.250000, loss: 0.213662\n",
      "Epoch: 1, iters: 20/28, pose loss: 0.202572, penalty: 3.625000, loss: 0.202681\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/data/home/jeholee/omni3D/deepvio/nuscenes_vio.ipynb Cell 13\u001b[0m line \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.140.76/data/home/jeholee/omni3D/deepvio/nuscenes_vio.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(message)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.140.76/data/home/jeholee/omni3D/deepvio/nuscenes_vio.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B165.132.140.76/data/home/jeholee/omni3D/deepvio/nuscenes_vio.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m avg_pose_loss, avg_penalty_loss \u001b[39m=\u001b[39m train(model, optimizer, train_loader, selection, temp, logger, ep, p\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n",
      "\u001b[1;32m/data/home/jeholee/omni3D/deepvio/nuscenes_vio.ipynb Cell 13\u001b[0m line \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, selection, temp, logger, ep, p, weighted)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.140.76/data/home/jeholee/omni3D/deepvio/nuscenes_vio.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m penalties \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.140.76/data/home/jeholee/omni3D/deepvio/nuscenes_vio.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m data_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B165.132.140.76/data/home/jeholee/omni3D/deepvio/nuscenes_vio.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (imgs, imus, gts, rot, weight) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.140.76/data/home/jeholee/omni3D/deepvio/nuscenes_vio.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     imgs \u001b[39m=\u001b[39m imgs\u001b[39m.\u001b[39mcuda()\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.140.76/data/home/jeholee/omni3D/deepvio/nuscenes_vio.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     imus \u001b[39m=\u001b[39m imus\u001b[39m.\u001b[39mcuda()\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1207\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1206\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1207\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1210\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m   1162\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[0;32m-> 1163\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1164\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1165\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1011\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m    999\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1012\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1013\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1014\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    178\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[1;32m    180\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[1;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    307\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create logs\n",
    "logger = logging.getLogger(args.experiment_name)\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger.info('----------------------------------------TRAINING----------------------------------')\n",
    "logger.info('PARAMETER ...')\n",
    "logger.info(args)\n",
    "\n",
    "# Load the dataset\n",
    "transform_train = [custom_transform.ToTensor(), custom_transform.Resize((args.img_h, args.img_w))]\n",
    "if args.hflip:\n",
    "    transform_train += [custom_transform.RandomHorizontalFlip()]\n",
    "if args.color:\n",
    "    transform_train += [custom_transform.RandomColorAug()]\n",
    "transform_train = custom_transform.Compose(transform_train)\n",
    "\n",
    "##############################################################\n",
    "max_imu_length = 11 # KITTI\n",
    "##############################################################\n",
    "\n",
    "train_dataset = Nusc_Dataset(dataroot,\n",
    "                             sequence_length=args.seq_len,\n",
    "                             max_imu_length=max_imu_length,\n",
    "                             transform=transform_train,\n",
    "                             nusc=nusc,\n",
    "                             nusc_can=nusc_can)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False, # TODO true?\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "# Model initialization\n",
    "model = DeepVIO(args)\n",
    "\n",
    "# Continual training or not\n",
    "if args.pretrain is not None:\n",
    "    model.load_state_dict(torch.load(args.pretrain))\n",
    "    print('load model %s'%args.pretrain)\n",
    "    logger.info('load model %s'%args.pretrain)\n",
    "else:\n",
    "    print('Training from scratch')\n",
    "    logger.info('Training from scratch')\n",
    "    \n",
    "# Use the pre-trained flownet or not\n",
    "if args.pretrain_flownet and args.pretrain is None:\n",
    "    pretrained_w = torch.load(args.pretrain_flownet, map_location='cpu')\n",
    "    model_dict = model.Feature_net.state_dict()\n",
    "    update_dict = {k: v for k, v in pretrained_w['state_dict'].items() if k in model_dict}\n",
    "    model_dict.update(update_dict)\n",
    "    model.Feature_net.load_state_dict(model_dict)\n",
    "\n",
    "# Feed model to GPU\n",
    "model.to(device)\n",
    "model = torch.nn.DataParallel(model, device_ids = [device])\n",
    "\n",
    "pretrain = args.pretrain \n",
    "if args.pretrain is None or pretrain[-5:] == 'model':\n",
    "    init_epoch = 0\n",
    "else:\n",
    "    init_epoch = int(pretrain[-7:-4])+1\n",
    "\n",
    "# Initialize the optimizer\n",
    "if args.optimizer == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "elif args.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, weight_decay=args.weight_decay)\n",
    "\n",
    "best = 10000\n",
    "\n",
    "for ep in range(init_epoch, args.epochs_warmup+args.epochs_joint+args.epochs_fine):\n",
    "    lr, selection, temp = update_status(ep, args, model)\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    message = f'Epoch: {ep}, lr: {lr}, selection: {selection}, temperaure: {temp:.5f}'\n",
    "    print(message)\n",
    "    logger.info(message)\n",
    "    \n",
    "    model.train()\n",
    "    avg_pose_loss, avg_penalty_loss = train(model, optimizer, train_loader, selection, temp, logger, ep, p=0.5)\n",
    "    \n",
    "    # Save the model after training\n",
    "    # torch.save(model.module.state_dict(), f'{checkpoints_dir}/{ep:003}.pth')\n",
    "    # message = f'Epoch {ep} training finished, pose loss: {avg_pose_loss:.6f}, penalty_loss: {avg_penalty_loss:.6f}, model saved'\n",
    "    # print(message)\n",
    "    # logger.info(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, imus, gts, rot, weight = train_dataset[0]\n",
    "imus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 3, 256, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
