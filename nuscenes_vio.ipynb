{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.can_bus.can_bus_api import NuScenesCanBus\n",
    "from nuscenes.utils import splits\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import pprint\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from path import Path\n",
    "from utils import custom_transform\n",
    "from dataset.KITTI_dataset import KITTI\n",
    "from dataset.NuScenes_dataset import NuScenes_Dataset\n",
    "from model import DeepVIO\n",
    "from collections import defaultdict\n",
    "from utils.kitti_eval import KITTI_tester, data_partition\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from utils.utils import *\n",
    "\n",
    "from utils.utils import rotationError, read_pose_from_text\n",
    "from collections import Counter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "from scipy.ndimage import convolve1d\n",
    "from torch.utils.data import Dataset\n",
    "from utils import custom_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "dataroot = '/data/public/360_3D_OD_Dataset/nuscenes'\n",
    "cam_names = [\"CAM_FRONT\", \"CAM_FRONT_RIGHT\", \"CAM_BACK_RIGHT\", \"CAM_BACK\", \"CAM_BACK_LEFT\", \"CAM_FRONT_LEFT\"]\n",
    "#########################################################################################\n",
    "\n",
    "nusc_can = NuScenesCanBus(dataroot=dataroot)\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot=dataroot, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3,4\"  # Set the GPUs 2 and 3 to use\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = '3, 4'\n",
    "# device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--save_dir', type=str, default='./results', help='path to save the result')\n",
    "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
    "\n",
    "# jeho\n",
    "# parser.add_argument('--img_w', type=int, default=512, help='image width')\n",
    "# parser.add_argument('--img_h', type=int, default=256, help='image height')\n",
    "parser.add_argument('--img_w', type=int, default=448, help='image width')\n",
    "parser.add_argument('--img_h', type=int, default=256, help='image height')\n",
    "\n",
    "parser.add_argument('--v_f_len', type=int, default=512, help='visual feature length')\n",
    "parser.add_argument('--i_f_len', type=int, default=256, help='imu feature length')\n",
    "parser.add_argument('--fuse_method', type=str, default='cat', help='fusion method [cat, soft, hard]')\n",
    "parser.add_argument('--imu_dropout', type=float, default=0, help='dropout for the IMU encoder')\n",
    "parser.add_argument('--rnn_hidden_size', type=int, default=1024, help='size of the LSTM latent')\n",
    "parser.add_argument('--rnn_dropout_out', type=float, default=0.2, help='dropout for the LSTM output layer')\n",
    "parser.add_argument('--rnn_dropout_between', type=float, default=0.2, help='dropout within LSTM')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-6, help='weight decay for the optimizer')\n",
    "\n",
    "parser.add_argument('--seq_len', type=int, default=11, help='sequence length for LSTM')\n",
    "parser.add_argument('--workers', type=int, default=4, help='number of workers')\n",
    "\n",
    "# jeho\n",
    "# NuScenes - 68,000 training samples, total 25 epochs -> 1,700,000 iterations assuming batch size 1\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--epochs_warmup', type=int, default=5, help='number of epochs for warmup')\n",
    "parser.add_argument('--epochs_joint', type=int, default=15, help='number of epochs for joint training')\n",
    "parser.add_argument('--epochs_fine', type=int, default=5, help='number of epochs for finetuning')\n",
    "\n",
    "# KITTI - 17,000 training samples, total 100 epochs -> 1,700,000 iterations assuming batch size 1\n",
    "# parser.add_argument('--epochs_warmup', type=int, default=40, help='number of epochs for warmup')\n",
    "# parser.add_argument('--epochs_joint', type=int, default=40, help='number of epochs for joint training')\n",
    "# parser.add_argument('--epochs_fine', type=int, default=20, help='number of epochs for finetuning')\n",
    "\n",
    "\n",
    "parser.add_argument('--lr_warmup', type=float, default=5e-4, help='learning rate for warming up stage')\n",
    "parser.add_argument('--lr_joint', type=float, default=5e-5, help='learning rate for joint training stage')\n",
    "parser.add_argument('--lr_fine', type=float, default=1e-6, help='learning rate for finetuning stage')\n",
    "parser.add_argument('--eta', type=float, default=0.05, help='exponential decay factor for temperature')\n",
    "parser.add_argument('--temp_init', type=float, default=5, help='initial temperature for gumbel-softmax')\n",
    "parser.add_argument('--Lambda', type=float, default=3e-5, help='penalty factor for the visual encoder usage')\n",
    "\n",
    "parser.add_argument('--experiment_name', type=str, default='experiment', help='experiment name')\n",
    "parser.add_argument('--optimizer', type=str, default='Adam', help='type of optimizer [Adam, SGD]')\n",
    "\n",
    "parser.add_argument('--pretrain_flownet',type=str, default='./pretrained_models/flownets_bn_EPE2.459.pth.tar', help='wehther to use the pre-trained flownet')\n",
    "parser.add_argument('--pretrain', type=str, default=None, help='path to the pretrained model')\n",
    "parser.add_argument('--hflip', default=False, action='store_true', help='whether to use horizonal flipping as augmentation')\n",
    "parser.add_argument('--color', default=False, action='store_true', help='whether to use color augmentations')\n",
    "\n",
    "parser.add_argument('--print_frequency', type=int, default=10, help='print frequency for loss values')\n",
    "parser.add_argument('--weighted', default=False, action='store_true', help='whether to use weighted sum')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "def update_status(ep, args, model):\n",
    "    if ep < args.epochs_warmup:  # Warmup stage\n",
    "        lr = args.lr_warmup\n",
    "        selection = 'random'\n",
    "        temp = args.temp_init\n",
    "        for param in model.module.Policy_net.parameters(): # Disable the policy network\n",
    "            param.requires_grad = False\n",
    "    elif ep >= args.epochs_warmup and ep < args.epochs_warmup + args.epochs_joint: # Joint training stage\n",
    "        lr = args.lr_joint\n",
    "        selection = 'gumbel-softmax'\n",
    "        temp = args.temp_init * math.exp(-args.eta * (ep-args.epochs_warmup))\n",
    "        for param in model.module.Policy_net.parameters(): # Enable the policy network\n",
    "            param.requires_grad = True\n",
    "    elif ep >= args.epochs_warmup + args.epochs_joint: # Finetuning stage\n",
    "        lr = args.lr_fine\n",
    "        selection = 'gumbel-softmax'\n",
    "        temp = args.temp_init * math.exp(-args.eta * (ep-args.epochs_warmup))\n",
    "    return lr, selection, temp\n",
    "\n",
    "def train(model, optimizer, train_loader, selection, temp, logger, ep, p=0.5, weighted=False):\n",
    "    \n",
    "    mse_losses = []\n",
    "    penalties = []\n",
    "    data_len = len(train_loader)\n",
    "\n",
    "    for i, (imgs, imus, gts, rot, weight) in enumerate(train_loader):\n",
    "\n",
    "        imgs = imgs.cuda().float()\n",
    "        imus = imus.cuda().float()\n",
    "        gts = gts.cuda().float() \n",
    "        weight = weight.cuda().float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        poses, decisions, probs, _ = model(imgs, imus, is_first=True, hc=None, temp=temp, selection=selection, p=p)\n",
    "        \n",
    "        if not weighted:\n",
    "            angle_loss = torch.nn.functional.mse_loss(poses[:,:,:3], gts[:, :, :3])\n",
    "            translation_loss = torch.nn.functional.mse_loss(poses[:,:,3:], gts[:, :, 3:])\n",
    "        else:\n",
    "            weight = weight/weight.sum()\n",
    "            angle_loss = (weight.unsqueeze(-1).unsqueeze(-1) * (poses[:,:,:3] - gts[:, :, :3]) ** 2).mean()\n",
    "            translation_loss = (weight.unsqueeze(-1).unsqueeze(-1) * (poses[:,:,3:] - gts[:, :, 3:]) ** 2).mean()\n",
    "        \n",
    "        pose_loss = 100 * angle_loss + translation_loss        \n",
    "        penalty = (decisions[:,:,0].float()).sum(-1).mean()\n",
    "        loss = pose_loss + args.Lambda * penalty \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % args.print_frequency == 0: \n",
    "            message = f'Epoch: {ep}, iters: {i}/{data_len}, pose loss: {pose_loss.item():.6f}, penalty: {penalty.item():.6f}, loss: {loss.item():.6f}'\n",
    "            print(message)\n",
    "            logger.info(message)\n",
    "\n",
    "        mse_losses.append(pose_loss.item())\n",
    "        penalties.append(penalty.item())\n",
    "\n",
    "    return np.mean(mse_losses), np.mean(penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmcv.mkdir_or_exist(args.save_dir)\n",
    "checkpoints_dir = os.path.join(args.save_dir, \"experiment_1\")\n",
    "mmcv.mkdir_or_exist(checkpoints_dir)\n",
    "\n",
    "# Create logs\n",
    "logger = logging.getLogger(args.experiment_name)\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger.info('----------------------------------------TRAINING----------------------------------')\n",
    "logger.info('PARAMETER ...')\n",
    "logger.info(args)\n",
    "\n",
    "# Load the dataset\n",
    "transform_train = [custom_transform.ToTensor(), custom_transform.Resize((args.img_h, args.img_w))]\n",
    "if args.hflip:\n",
    "    transform_train += [custom_transform.RandomHorizontalFlip()]\n",
    "if args.color:\n",
    "    transform_train += [custom_transform.RandomColorAug()]\n",
    "transform_train = custom_transform.Compose(transform_train)\n",
    "\n",
    "##############################################################\n",
    "max_imu_length = 11 # KITTI\n",
    "##############################################################\n",
    "\n",
    "train_dataset = NuScenes_Dataset(dataroot,\n",
    "                             sequence_length=args.seq_len,\n",
    "                             max_imu_length=max_imu_length,\n",
    "                             cam_names=cam_names,\n",
    "                             transform=transform_train,\n",
    "                             nusc=nusc,\n",
    "                             nusc_can=nusc_can)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True, # TODO false?\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU selections\n",
    "str_ids = device.split(',')\n",
    "gpu_ids = []\n",
    "for str_id in str_ids:\n",
    "    id = int(str_id)\n",
    "    if id >= 0:\n",
    "        gpu_ids.append(id)\n",
    "if len(gpu_ids) > 0:\n",
    "    torch.cuda.set_device(gpu_ids[0])\n",
    "\n",
    "# Initialize the tester\n",
    "tester = KITTI_tester(args)\n",
    "    \n",
    "# Model initialization\n",
    "model = DeepVIO(args)\n",
    "\n",
    "# Continual training or not\n",
    "if args.pretrain is not None:\n",
    "    model.load_state_dict(torch.load(args.pretrain))\n",
    "    print('load model %s'%args.pretrain)\n",
    "    logger.info('load model %s'%args.pretrain)\n",
    "else:\n",
    "    print('Training from scratch')\n",
    "    logger.info('Training from scratch')\n",
    "\n",
    "# Use the pre-trained flownet or not\n",
    "if args.pretrain_flownet and args.pretrain is None:\n",
    "    pretrained_w = torch.load(args.pretrain_flownet, map_location='cpu')\n",
    "    model_dict = model.Feature_net.state_dict()\n",
    "    update_dict = {k: v for k, v in pretrained_w['state_dict'].items() if k in model_dict}\n",
    "    model_dict.update(update_dict)\n",
    "    model.Feature_net.load_state_dict(model_dict)\n",
    "\n",
    "# Feed model to GPU\n",
    "# model.to(device)\n",
    "# model = torch.nn.DataParallel(model, device_ids = [device])\n",
    "\n",
    "# model = model.cuda()\n",
    "model.cuda(gpu_ids[0])\n",
    "model = torch.nn.DataParallel(model, device_ids = gpu_ids)\n",
    "\n",
    "pretrain = args.pretrain \n",
    "if args.pretrain is None or pretrain[-5:] == 'model':\n",
    "    init_epoch = 0\n",
    "else:\n",
    "    init_epoch = int(pretrain[-7:-4])+1\n",
    "\n",
    "# Initialize the optimizer\n",
    "if args.optimizer == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "elif args.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, weight_decay=args.weight_decay)\n",
    "\n",
    "best = 10000\n",
    "\n",
    "for ep in range(init_epoch, args.epochs_warmup+args.epochs_joint+args.epochs_fine):\n",
    "    lr, selection, temp = update_status(ep, args, model)\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    message = f'Epoch: {ep}, lr: {lr}, selection: {selection}, temperaure: {temp:.5f}'\n",
    "    print(message)\n",
    "    logger.info(message)\n",
    "    \n",
    "    model.train()\n",
    "    avg_pose_loss, avg_penalty_loss = train(model, optimizer, train_loader, selection, temp, logger, ep, p=0.5)\n",
    "    \n",
    "    if ep > args.epochs_warmup+args.epochs_joint:\n",
    "        # Save the model after training\n",
    "        torch.save(model.module.state_dict(), f'{checkpoints_dir}/{ep:003}.pth')\n",
    "        message = f'Epoch {ep} training finished, pose loss: {avg_pose_loss:.6f}, penalty_loss: {avg_penalty_loss:.6f}, model saved'\n",
    "        print(message)\n",
    "        logger.info(message)\n",
    "    \n",
    "        # Evaluate the model\n",
    "        # print('Evaluating the model')\n",
    "        # logger.info('Evaluating the model')\n",
    "        # with torch.no_grad(): \n",
    "        #     model.eval()\n",
    "        #     errors = tester.eval(model, selection='gumbel-softmax', num_gpu=len(gpu_ids))\n",
    "    \n",
    "        # t_rel = np.mean([errors[i]['t_rel'] for i in range(len(errors))])\n",
    "        # r_rel = np.mean([errors[i]['r_rel'] for i in range(len(errors))])\n",
    "        # t_rmse = np.mean([errors[i]['t_rmse'] for i in range(len(errors))])\n",
    "        # r_rmse = np.mean([errors[i]['r_rmse'] for i in range(len(errors))])\n",
    "        # usage = np.mean([errors[i]['usage'] for i in range(len(errors))])\n",
    "\n",
    "        # if t_rel < best:\n",
    "        #     best = t_rel \n",
    "        #     torch.save(model.module.state_dict(), f'{checkpoints_dir}/best_{best:.2f}.pth')\n",
    "    \n",
    "        # message = f'Epoch {ep} evaluation finished , t_rel: {t_rel:.4f}, r_rel: {r_rel:.4f}, t_rmse: {t_rmse:.4f}, r_rmse: {r_rmse:.4f}, usage: {usage:.4f}, best t_rel: {best:.4f}'\n",
    "        # logger.info(message)\n",
    "        # print(message)\n",
    "\n",
    "message = f'Training finished, best t_rel: {best:.4f}'\n",
    "logger.info(message)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
