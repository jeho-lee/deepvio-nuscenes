{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.can_bus.can_bus_api import NuScenesCanBus\n",
    "from nuscenes.utils import splits\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import pprint\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from path import Path\n",
    "from utils import custom_transform\n",
    "from dataset.KITTI_dataset import KITTI\n",
    "from dataset.NuScenes_dataset import NuScenes_Dataset\n",
    "from model import DeepVIO\n",
    "from collections import defaultdict\n",
    "from utils.kitti_eval import KITTI_tester, data_partition\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from utils.utils import *\n",
    "\n",
    "from utils.utils import rotationError, read_pose_from_text\n",
    "from collections import Counter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "from scipy.ndimage import convolve1d\n",
    "from torch.utils.data import Dataset\n",
    "from utils import custom_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_rotation_matrix(Q):\n",
    "    \"\"\"\n",
    "    Covert a quaternion into a full three-dimensional rotation matrix.\n",
    " \n",
    "    Input\n",
    "    :param Q: A 4 element array representing the quaternion (q0,q1,q2,q3) \n",
    " \n",
    "    Output\n",
    "    :return: A 3x3 element matrix representing the full 3D rotation matrix. \n",
    "             This rotation matrix converts a point in the local reference \n",
    "             frame to a point in the global reference frame.\n",
    "    \"\"\"\n",
    "    # Extract the values from Q\n",
    "    q0 = Q[0]\n",
    "    q1 = Q[1]\n",
    "    q2 = Q[2]\n",
    "    q3 = Q[3]\n",
    "     \n",
    "    # First row of the rotation matrix\n",
    "    r00 = 2 * (q0 * q0 + q1 * q1) - 1\n",
    "    r01 = 2 * (q1 * q2 - q0 * q3)\n",
    "    r02 = 2 * (q1 * q3 + q0 * q2)\n",
    "     \n",
    "    # Second row of the rotation matrix\n",
    "    r10 = 2 * (q1 * q2 + q0 * q3)\n",
    "    r11 = 2 * (q0 * q0 + q2 * q2) - 1\n",
    "    r12 = 2 * (q2 * q3 - q0 * q1)\n",
    "     \n",
    "    # Third row of the rotation matrix\n",
    "    r20 = 2 * (q1 * q3 - q0 * q2)\n",
    "    r21 = 2 * (q2 * q3 + q0 * q1)\n",
    "    r22 = 2 * (q0 * q0 + q3 * q3) - 1\n",
    "     \n",
    "    # 3x3 rotation matrix\n",
    "    rot_matrix = np.array([[r00, r01, r02],\n",
    "                           [r10, r11, r12],\n",
    "                           [r20, r21, r22]])\n",
    "                            \n",
    "    return rot_matrix\n",
    "\n",
    "def euler_from_matrix(matrix):\n",
    "    '''\n",
    "    Extract the eular angle from a rotation matrix\n",
    "    '''\n",
    "    _EPS = np.finfo(float).eps * 4.0\n",
    "    \n",
    "    M = np.array(matrix, dtype=np.float64, copy=False)[:3, :3]\n",
    "    cy = math.sqrt(M[0, 0] * M[0, 0] + M[1, 0] * M[1, 0])\n",
    "    ay = math.atan2(-M[2, 0], cy)\n",
    "    if ay < -math.pi / 2 + _EPS and ay > -math.pi / 2 - _EPS:  # pitch = -90 deg\n",
    "        ax = 0\n",
    "        az = math.atan2(-M[1, 2], -M[0, 2])\n",
    "    elif ay < math.pi / 2 + _EPS and ay > math.pi / 2 - _EPS:\n",
    "        ax = 0\n",
    "        az = math.atan2(M[1, 2], M[0, 2])\n",
    "    else:\n",
    "        ax = math.atan2(M[2, 1], M[2, 2])\n",
    "        az = math.atan2(M[1, 0], M[0, 0])\n",
    "    return np.array([ax, ay, az])\n",
    "\n",
    "def get_lds_kernel_window(kernel, ks, sigma):\n",
    "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "    half_ks = (ks - 1) // 2\n",
    "    if kernel == 'gaussian':\n",
    "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "    elif kernel == 'triang':\n",
    "        kernel_window = triang(ks)\n",
    "    else:\n",
    "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "    return kernel_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NuScenes_Val_Dataset(Dataset):\n",
    "    def __init__(self, img_path_list, pose_rel_list, imu_list, args):\n",
    "        super(NuScenes_Val_Dataset, self).__init__()\n",
    "        self.img_path_list = img_path_list\n",
    "        self.pose_rel_list = pose_rel_list\n",
    "        self.imu_list = imu_list\n",
    "        self.args = args\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_path_sequence = self.img_path_list[index]\n",
    "        image_sequence = []\n",
    "        for img_path in image_path_sequence:\n",
    "            img_as_img = Image.open(img_path)\n",
    "            img_as_img = TF.resize(img_as_img, size=(self.args.img_h, self.args.img_w))\n",
    "            img_as_tensor = TF.to_tensor(img_as_img) - 0.5\n",
    "            img_as_tensor = img_as_tensor.unsqueeze(0)\n",
    "            image_sequence.append(img_as_tensor)\n",
    "        image_sequence = torch.cat(image_sequence, 0)\n",
    "        gt_sequence = self.pose_rel_list[index][:, :6]\n",
    "        imu_sequence = torch.FloatTensor(self.imu_list[index])\n",
    "        return image_sequence, imu_sequence, gt_sequence\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "\n",
    "class NuScenes_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_root,\n",
    "                 mode='train', # or 'val'\n",
    "                 sequence_length=11,\n",
    "                 max_imu_length=10,\n",
    "                 cam_names = [\"CAM_FRONT\", \"CAM_FRONT_RIGHT\", \"CAM_BACK_RIGHT\", \"CAM_BACK\", \"CAM_BACK_LEFT\", \"CAM_FRONT_LEFT\"],\n",
    "                 transform=None,\n",
    "                 nusc=None,\n",
    "                 nusc_can=None,\n",
    "                 args=None):\n",
    "        self.data_root = data_root\n",
    "        if nusc is None:\n",
    "            self.nusc = NuScenes(version='v1.0-trainval', dataroot=self.data_root, verbose=False)\n",
    "        else:\n",
    "            self.nusc = nusc\n",
    "        if nusc_can is None:\n",
    "            self.nusc_can = NuScenesCanBus(dataroot=self.data_root)\n",
    "        else:\n",
    "            self.nusc_can = nusc_can\n",
    "        self.cam_names = cam_names\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_imu_length = max_imu_length\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.make_train_dataset()\n",
    "        self.args = args\n",
    "    \n",
    "    def get_available_scene_tokens(self):\n",
    "        \"\"\"Code from bevdet codebase - tools/data_converter/nuscenes_converter.py\"\"\"\n",
    "        train_scenes = splits.train\n",
    "        val_scenes = splits.val\n",
    "\n",
    "        available_scenes = []\n",
    "        for scene in self.nusc.scene:\n",
    "            scene_token = scene['token']\n",
    "            scene_rec = self.nusc.get('scene', scene_token)\n",
    "            sample_rec = self.nusc.get('sample', scene_rec['first_sample_token'])\n",
    "            sd_rec = self.nusc.get('sample_data', sample_rec['data']['LIDAR_TOP'])\n",
    "            has_more_frames = True\n",
    "            scene_not_exist = False\n",
    "            while has_more_frames:\n",
    "                lidar_path, boxes, _ = self.nusc.get_sample_data(sd_rec['token'])\n",
    "                lidar_path = str(lidar_path)\n",
    "                if os.getcwd() in lidar_path:\n",
    "                    # path from lyftdataset is absolute path\n",
    "                    lidar_path = lidar_path.split(f'{os.getcwd()}/')[-1]\n",
    "                    # relative path\n",
    "                if not mmcv.is_filepath(lidar_path):\n",
    "                    scene_not_exist = True\n",
    "                    break\n",
    "                else:\n",
    "                    break\n",
    "            if scene_not_exist:\n",
    "                continue\n",
    "            available_scenes.append(scene)\n",
    "\n",
    "        available_scene_names = [s['name'] for s in available_scenes]\n",
    "        train_scenes = list(filter(lambda x: x in available_scene_names, train_scenes))\n",
    "        val_scenes = list(filter(lambda x: x in available_scene_names, val_scenes))\n",
    "        \n",
    "        train_scenes = set([\n",
    "            available_scenes[available_scene_names.index(s)]['token']\n",
    "            for s in train_scenes\n",
    "        ])\n",
    "        val_scenes = set([\n",
    "            available_scenes[available_scene_names.index(s)]['token']\n",
    "            for s in val_scenes\n",
    "        ])\n",
    "        \n",
    "        train_scenes = [self.nusc.get('scene', token) for token in train_scenes]\n",
    "        val_scenes = [self.nusc.get('scene', token) for token in val_scenes]\n",
    "        \n",
    "        return train_scenes, val_scenes\n",
    "    \n",
    "    def get_scene_data(self, scene_record, cam_name):\n",
    "        scene_name = scene_record['name']\n",
    "\n",
    "        # Get images and poses of target scene\n",
    "        first_sample_token = scene_record['first_sample_token']\n",
    "        cur_sample = self.nusc.get('sample', first_sample_token)\n",
    "        cur_sample_data = self.nusc.get('sample_data', cur_sample['data'][cam_name])\n",
    "\n",
    "        scene_sample_data = []\n",
    "        while True:\n",
    "            try:\n",
    "                scene_sample_data.append(cur_sample_data)\n",
    "                cur_sample_data = self.nusc.get('sample_data', cur_sample_data['next'])\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        scene_imu_data = self.nusc_can.get_messages(scene_name, 'ms_imu')\n",
    "        \n",
    "        return scene_sample_data, scene_imu_data\n",
    "    \n",
    "    def format_scene_inputs(self, scene_sample_data, scene_imu_data):\n",
    "        \"\"\" Collect image (12hz), pose (12hz), imu data (96hz) of target scene - single training input contains 2 images,  \"\"\"\n",
    "        # 1. 일단 각 scene input 모으기 - 2 images, 2 pose, 1 relative pose, 8 imu data\n",
    "        scene_inputs = []\n",
    "        for data_idx, cur_sample_data in enumerate(scene_sample_data):\n",
    "            \n",
    "            # 1. get image \n",
    "            cur_img_path = os.path.join(self.data_root, cur_sample_data['filename'])\n",
    "            if cur_sample_data['next'] != \"\":\n",
    "                next_sample_data = self.nusc.get('sample_data', cur_sample_data['next'])\n",
    "                next_img_path = os.path.join(self.data_root, next_sample_data['filename'])\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "            # 2. get ego pose\n",
    "            # read_pose in utils.py\n",
    "            cur_ego_pose = self.nusc.get('ego_pose', cur_sample_data['ego_pose_token'])\n",
    "            trans = np.array(cur_ego_pose['translation'])\n",
    "            trans = trans.reshape(3, -1)\n",
    "            rot_mat = quaternion_rotation_matrix(cur_ego_pose['rotation']) # (w, x, y, z)\n",
    "            cur_ego_pose_mat = np.concatenate((rot_mat, trans), axis=1)\n",
    "            cur_ego_pose_mat = np.array(cur_ego_pose_mat).reshape(3, 4)\n",
    "            cur_ego_pose_mat = np.concatenate((cur_ego_pose_mat, np.array([[0, 0, 0, 1]])), 0)\n",
    "            \n",
    "            next_ego_pose = self.nusc.get('ego_pose', next_sample_data['ego_pose_token'])\n",
    "            trans = np.array(next_ego_pose['translation'])\n",
    "            trans = trans.reshape(3, -1)\n",
    "            rot_mat = quaternion_rotation_matrix(next_ego_pose['rotation']) # (w, x, y, z)\n",
    "            next_ego_pose_mat = np.concatenate((rot_mat, trans), axis=1)\n",
    "            next_ego_pose_mat = np.array(next_ego_pose_mat).reshape(3, 4)\n",
    "            next_ego_pose_mat = np.concatenate((next_ego_pose_mat, np.array([[0, 0, 0, 1]])), 0)    \n",
    "\n",
    "            # 3. get relative pose\n",
    "            relative_pose = np.dot(np.linalg.inv(cur_ego_pose_mat), next_ego_pose_mat)\n",
    "            R_rel = relative_pose[:3, :3]\n",
    "            t_rel = relative_pose[:3, 3]\n",
    "\n",
    "                # Extract the Eular angle from the relative rotation matrix\n",
    "            x, y, z = euler_from_matrix(R_rel)\n",
    "            theta = [x, y, z]\n",
    "\n",
    "            pose_rel = np.concatenate((theta, t_rel))\n",
    "            \n",
    "            # 4. get imu data\n",
    "            cur_timestamp = cur_sample_data['timestamp']\n",
    "            next_timestamp = next_sample_data['timestamp']\n",
    "            \n",
    "            # get imu data between cur and next timestamp\n",
    "            imu_data = []\n",
    "            for imu in scene_imu_data:\n",
    "                imu_timestamp = imu['utime']\n",
    "                if imu_timestamp > cur_timestamp and imu_timestamp < next_timestamp:\n",
    "                    data = imu['linear_accel'] + imu['rotation_rate']\n",
    "                    imu_data.append(data)\n",
    "            \n",
    "            # if no matched imu data, skip\n",
    "            if len(imu_data) <= 2:\n",
    "                # continue\n",
    "                return None\n",
    "                \n",
    "            # if imu data length is less than max_imu_length, pad with zeros\n",
    "            if len(imu_data) < self.max_imu_length:\n",
    "                imu_data = np.pad(imu_data, ((0, self.max_imu_length - len(imu_data)), (0, 0)), 'constant', constant_values=0)\n",
    "            else:\n",
    "                imu_data = imu_data[:self.max_imu_length]\n",
    "            \n",
    "            # 5. make training input\n",
    "            training_input = {\n",
    "                'cur_img_path': cur_img_path,\n",
    "                'next_img_path': next_img_path,\n",
    "                'cur_ego_pose': cur_ego_pose_mat,\n",
    "                'next_ego_pose': next_ego_pose_mat,\n",
    "                'pose_rel': pose_rel,\n",
    "                'imu_data': imu_data\n",
    "            }\n",
    "            scene_inputs.append(training_input)\n",
    "        return scene_inputs\n",
    "    \n",
    "    def segment_training_inputs(self, training_inputs):\n",
    "        samples = []\n",
    "\n",
    "        input_idx = 0\n",
    "        while True:\n",
    "            # get training input chunk of sequence_length\n",
    "            training_input_chunk = training_inputs[input_idx : input_idx + (self.sequence_length-1)]\n",
    "            input_idx += 1 # training sequence간 겹치는 images 존재함\n",
    "            if len(training_input_chunk) < (self.sequence_length-1):\n",
    "                break\n",
    "            \n",
    "            img_samples = []\n",
    "            pose_samples = []\n",
    "            for training_input in training_input_chunk:\n",
    "                img_samples.append(training_input['cur_img_path'])\n",
    "                pose_samples.append(training_input['cur_ego_pose'])\n",
    "            img_samples.append(training_input_chunk[-1]['next_img_path'])\n",
    "            pose_samples.append(training_input_chunk[-1]['next_ego_pose'])\n",
    "            \n",
    "            pose_rel_samples = []\n",
    "            imu_samples = np.empty((0, 6))\n",
    "            for training_input in training_input_chunk:\n",
    "                pose_rel_samples.append(training_input['pose_rel'])\n",
    "                imu_samples = np.vstack((imu_samples, np.array(training_input['imu_data'])))\n",
    "            \n",
    "            pose_samples = np.array(pose_samples)\n",
    "            pose_rel_samples = np.array(pose_rel_samples)\n",
    "            imu_samples = np.array(imu_samples)\n",
    "    \n",
    "            segment_rot = rotationError(pose_samples[0], pose_samples[-1])\n",
    "            sample = {'imgs':img_samples, 'imus':imu_samples, 'gts': pose_rel_samples, 'rot': segment_rot}\n",
    "            \n",
    "            samples.append(sample)\n",
    "            \n",
    "        # Generate weights based on the rotation of the training segments\n",
    "        # Weights are calculated based on the histogram of rotations according to the method in https://github.com/YyzHarry/imbalanced-regression\n",
    "        rot_list = np.array([np.cbrt(item['rot']*180/np.pi) for item in samples])\n",
    "        rot_range = np.linspace(np.min(rot_list), np.max(rot_list), num=10)\n",
    "        indexes = np.digitize(rot_list, rot_range, right=False)\n",
    "        num_samples_of_bins = dict(Counter(indexes))\n",
    "        emp_label_dist = [num_samples_of_bins.get(i, 0) for i in range(1, len(rot_range)+1)]\n",
    "\n",
    "        # Apply 1d convolution to get the smoothed effective label distribution\n",
    "        lds_kernel_window = get_lds_kernel_window(kernel='gaussian', ks=7, sigma=5)\n",
    "        eff_label_dist = convolve1d(np.array(emp_label_dist), weights=lds_kernel_window, mode='constant')\n",
    "\n",
    "        weights = [np.float32(1/eff_label_dist[bin_idx-1]) for bin_idx in indexes]\n",
    "        \n",
    "        assert len(samples) == len(weights)\n",
    "        \n",
    "        return samples, weights\n",
    "    \n",
    "    def segment_val_inputs(self, scene_inputs):\n",
    "        img_samples, pose_rel_samples, imu_samples = [], [], []\n",
    "        input_idx = 0\n",
    "        while True:\n",
    "            val_input_chunk = scene_inputs[input_idx : input_idx + (self.sequence_length - 1)]\n",
    "            input_idx = input_idx + (self.sequence_length - 1)\n",
    "            if len(val_input_chunk) < (self.sequence_length-1):\n",
    "                break\n",
    "            \n",
    "            imgs = []\n",
    "            for val_input in val_input_chunk:\n",
    "                imgs.append(val_input['cur_img_path'])\n",
    "            imgs.append(val_input_chunk[-1]['next_img_path'])\n",
    "            \n",
    "            pose_rels = []\n",
    "            imus = np.empty((0, 6))\n",
    "            for val_input in val_input_chunk:\n",
    "                pose_rels.append(val_input['pose_rel'])\n",
    "                imus = np.vstack((imus, np.array(val_input['imu_data'])))\n",
    "\n",
    "            img_samples.append(imgs)\n",
    "            pose_rel_samples.append(np.array(pose_rels))\n",
    "            imu_samples.append(np.array(imus))\n",
    "\n",
    "        return img_samples, pose_rel_samples, imu_samples\n",
    "                \n",
    "    def filter_dataset(self, scenes):\n",
    "        skipped_scene = []\n",
    "        imuavail_scenes = []\n",
    "        for idx, train_scene in enumerate(scenes):\n",
    "            scene_name = train_scene['name']\n",
    "            scene_idx = int(scene_name.split('-')[-1])\n",
    "            if scene_idx in self.nusc_can.route_blacklist or scene_idx in self.nusc_can.can_blacklist: # skip if scene has no can_bus data\n",
    "                skipped_scene.append(scene_name)\n",
    "                continue\n",
    "            imuavail_scenes.append(train_scene)\n",
    "        \n",
    "        target_scenes = []\n",
    "        for idx, train_scene in enumerate(imuavail_scenes):\n",
    "            avail_cam_num = 0\n",
    "            for cam_name in self.cam_names:\n",
    "                scene_sample_data, scene_imu_data = self.get_scene_data(train_scene, cam_name)\n",
    "                scene_inputs = self.format_scene_inputs(scene_sample_data, scene_imu_data)\n",
    "                if scene_inputs is None: # skip if there are any scene samples that have no associated imu data\n",
    "                    break\n",
    "                avail_cam_num += 1\n",
    "            if avail_cam_num == len(self.cam_names):\n",
    "                target_scenes.append(train_scene)\n",
    "            else:\n",
    "                skipped_scene.append(train_scene['name'])\n",
    "        print('skipped scenes: {}'.format(len(skipped_scene)))\n",
    "        return target_scenes\n",
    "    \n",
    "    def make_train_dataset(self):\n",
    "        train_scenes, val_scenes = self.get_available_scene_tokens()\n",
    "        target_train_scenes = self.filter_dataset(train_scenes)\n",
    "        \n",
    "        self.samples, self.weights = [], []\n",
    "        for idx, train_scene in enumerate(target_train_scenes):\n",
    "            \n",
    "            # select camera one by one\n",
    "            cam_name = self.cam_names[idx % len(self.cam_names)]\n",
    "            \n",
    "            # collect samples and weights                \n",
    "            scene_sample_data, scene_imu_data = self.get_scene_data(train_scene, cam_name)\n",
    "            scene_training_inputs = self.format_scene_inputs(scene_sample_data, scene_imu_data)\n",
    "            scene_samples, scene_weights = self.segment_training_inputs(scene_training_inputs)\n",
    "            self.samples.extend(scene_samples)\n",
    "            self.weights.extend(scene_weights)\n",
    "        \n",
    "        print('total samples: {}'.format(len(self.samples)))\n",
    "        assert len(self.samples) == len(self.weights)\n",
    "    \n",
    "    def get_val_dataset(self):\n",
    "        _, val_scenes = self.get_available_scene_tokens()\n",
    "        target_val_scenes = self.filter_dataset(val_scenes)\n",
    "        \n",
    "        total_samples_num = 0\n",
    "        val_scene_datasets = []\n",
    "        for idx, val_scene in enumerate(target_val_scenes):\n",
    "            img_path_list, pose_rel_list, imu_list = [], [], []\n",
    "            \n",
    "            \"\"\"\n",
    "            TODO\n",
    "            camera to ego transformation을 고려해야 하는지?\n",
    "            \"\"\"\n",
    "            # select camera one by one\n",
    "            cam_name = self.cam_names[idx % len(self.cam_names)]\n",
    "            # cam_name = \"CAM_FRONT\"\n",
    "            \n",
    "            scene_sample_data, scene_imu_data = self.get_scene_data(val_scene, cam_name)\n",
    "            scene_val_inputs = self.format_scene_inputs(scene_sample_data, scene_imu_data)\n",
    "            img_samples, pose_rel_samples, imu_samples = self.segment_val_inputs(scene_val_inputs)\n",
    "            \n",
    "            img_path_list.extend(img_samples)\n",
    "            pose_rel_list.extend(pose_rel_samples)\n",
    "            imu_list.extend(imu_samples)\n",
    "            \n",
    "            total_samples_num += len(img_path_list)\n",
    "            \n",
    "            val_scene_datasets.append(NuScenes_Val_Dataset(img_path_list, pose_rel_list, imu_list, self.args))\n",
    "            \n",
    "            # TEMP\n",
    "            # if idx == 2:\n",
    "            #     break\n",
    "\n",
    "        print('total samples: {}'.format(total_samples_num))\n",
    "        \n",
    "        return val_scene_datasets\n",
    "    \n",
    "    # the Dataset class implementation only works for training set\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        imgs = [np.asarray(Image.open(img)) for img in sample['imgs']]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            # imgs, imus, gts = self.transform(imgs, np.copy(sample['imus']), np.copy(sample['gts']))\n",
    "            imgs, imus, gts = self.transform(imgs, np.copy(sample['imus']).astype(np.float32), np.copy(sample['gts']).astype(np.float32))\n",
    "        else:\n",
    "            imus = np.copy(sample['imus'])\n",
    "            gts = np.copy(sample['gts']).astype(np.float32)\n",
    "        \n",
    "        rot = sample['rot'].astype(np.float32)\n",
    "        weight = self.weights[index]\n",
    "\n",
    "        return imgs, imus, gts, rot, weight\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"7\"  # Set the GPUs 2 and 3 to use\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = '7'\n",
    "# device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "dataroot = '/data/public/360_3D_OD_Dataset/nuscenes'\n",
    "cam_names = [\"CAM_FRONT\", \"CAM_FRONT_RIGHT\", \"CAM_BACK_RIGHT\", \"CAM_BACK\", \"CAM_BACK_LEFT\", \"CAM_FRONT_LEFT\"]\n",
    "#########################################################################################\n",
    "\n",
    "nusc_can = NuScenesCanBus(dataroot=dataroot)\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot=dataroot, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--save_dir', type=str, default='./results', help='path to save the result')\n",
    "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
    "\n",
    "# jeho\n",
    "# parser.add_argument('--img_w', type=int, default=512, help='image width')\n",
    "# parser.add_argument('--img_h', type=int, default=256, help='image height')\n",
    "parser.add_argument('--img_w', type=int, default=448, help='image width')\n",
    "parser.add_argument('--img_h', type=int, default=256, help='image height')\n",
    "\n",
    "parser.add_argument('--v_f_len', type=int, default=512, help='visual feature length')\n",
    "parser.add_argument('--i_f_len', type=int, default=256, help='imu feature length')\n",
    "parser.add_argument('--fuse_method', type=str, default='cat', help='fusion method [cat, soft, hard]')\n",
    "parser.add_argument('--imu_dropout', type=float, default=0, help='dropout for the IMU encoder')\n",
    "parser.add_argument('--rnn_hidden_size', type=int, default=1024, help='size of the LSTM latent')\n",
    "parser.add_argument('--rnn_dropout_out', type=float, default=0.2, help='dropout for the LSTM output layer')\n",
    "parser.add_argument('--rnn_dropout_between', type=float, default=0.2, help='dropout within LSTM')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-6, help='weight decay for the optimizer')\n",
    "\n",
    "parser.add_argument('--seq_len', type=int, default=11, help='sequence length for LSTM')\n",
    "parser.add_argument('--workers', type=int, default=4, help='number of workers')\n",
    "\n",
    "# jeho\n",
    "# NuScenes - 68,000 training samples, total 25 epochs -> 1,700,000 iterations assuming batch size 1\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--epochs_warmup', type=int, default=5, help='number of epochs for warmup')\n",
    "parser.add_argument('--epochs_joint', type=int, default=15, help='number of epochs for joint training')\n",
    "parser.add_argument('--epochs_fine', type=int, default=5, help='number of epochs for finetuning')\n",
    "\n",
    "# KITTI - 17,000 training samples, total 100 epochs -> 1,700,000 iterations assuming batch size 1\n",
    "# parser.add_argument('--epochs_warmup', type=int, default=40, help='number of epochs for warmup')\n",
    "# parser.add_argument('--epochs_joint', type=int, default=40, help='number of epochs for joint training')\n",
    "# parser.add_argument('--epochs_fine', type=int, default=20, help='number of epochs for finetuning')\n",
    "\n",
    "\n",
    "parser.add_argument('--lr_warmup', type=float, default=5e-4, help='learning rate for warming up stage')\n",
    "parser.add_argument('--lr_joint', type=float, default=5e-5, help='learning rate for joint training stage')\n",
    "parser.add_argument('--lr_fine', type=float, default=1e-6, help='learning rate for finetuning stage')\n",
    "parser.add_argument('--eta', type=float, default=0.05, help='exponential decay factor for temperature')\n",
    "parser.add_argument('--temp_init', type=float, default=5, help='initial temperature for gumbel-softmax')\n",
    "parser.add_argument('--Lambda', type=float, default=3e-5, help='penalty factor for the visual encoder usage')\n",
    "\n",
    "parser.add_argument('--experiment_name', type=str, default='experiment', help='experiment name')\n",
    "parser.add_argument('--optimizer', type=str, default='Adam', help='type of optimizer [Adam, SGD]')\n",
    "\n",
    "parser.add_argument('--pretrain_flownet',type=str, default='./pretrained_models/flownets_bn_EPE2.459.pth.tar', help='wehther to use the pre-trained flownet')\n",
    "parser.add_argument('--pretrain', type=str, default=None, help='path to the pretrained model')\n",
    "parser.add_argument('--hflip', default=False, action='store_true', help='whether to use horizonal flipping as augmentation')\n",
    "parser.add_argument('--color', default=False, action='store_true', help='whether to use color augmentations')\n",
    "\n",
    "parser.add_argument('--print_frequency', type=int, default=10, help='print frequency for loss values')\n",
    "parser.add_argument('--weighted', default=False, action='store_true', help='whether to use weighted sum')\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def plotPath_2D(seq, poses_gt_mat, poses_est_mat, plot_path_dir, decision, speed, window_size):\n",
    "    \n",
    "    # Apply smoothing to the decision\n",
    "    decision = np.insert(decision, 0, 1)\n",
    "    decision = moving_average(decision, window_size)\n",
    "\n",
    "    fontsize_ = 10\n",
    "    plot_keys = [\"Ground Truth\", \"Ours\"]\n",
    "    start_point = [0, 0]\n",
    "    style_pred = 'b-'\n",
    "    style_gt = 'r-'\n",
    "    style_O = 'ko'\n",
    "\n",
    "    # get the value\n",
    "    x_gt = np.asarray([pose[0, 3] for pose in poses_gt_mat])\n",
    "    y_gt = np.asarray([pose[1, 3] for pose in poses_gt_mat])\n",
    "    z_gt = np.asarray([pose[2, 3] for pose in poses_gt_mat])\n",
    "\n",
    "    x_pred = np.asarray([pose[0, 3] for pose in poses_est_mat])\n",
    "    y_pred = np.asarray([pose[1, 3] for pose in poses_est_mat])\n",
    "    z_pred = np.asarray([pose[2, 3] for pose in poses_est_mat])\n",
    "\n",
    "    # Plot 2d trajectory estimation map\n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.gca()\n",
    "    plt.plot(x_gt, z_gt, style_gt, label=plot_keys[0])\n",
    "    plt.plot(x_pred, z_pred, style_pred, label=plot_keys[1])\n",
    "    plt.plot(start_point[0], start_point[1], style_O, label='Start Point')\n",
    "    plt.legend(loc=\"upper right\", prop={'size': fontsize_})\n",
    "    plt.xlabel('x (m)', fontsize=fontsize_)\n",
    "    plt.ylabel('z (m)', fontsize=fontsize_)\n",
    "    # set the range of x and y\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    xmean = np.mean(xlim)\n",
    "    ymean = np.mean(ylim)\n",
    "    plot_radius = max([abs(lim - mean_)\n",
    "                       for lims, mean_ in ((xlim, xmean),\n",
    "                                           (ylim, ymean))\n",
    "                       for lim in lims])\n",
    "    ax.set_xlim([xmean - plot_radius, xmean + plot_radius])\n",
    "    ax.set_ylim([ymean - plot_radius, ymean + plot_radius])\n",
    "\n",
    "    plt.title('2D path')\n",
    "    png_title = \"{}_path_2d\".format(seq)\n",
    "    plt.savefig(plot_path_dir + \"/\" + png_title + \".png\", bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Plot 2d xy trajectory estimation map\n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.gca()\n",
    "    plt.plot(x_gt, y_gt, style_gt, label=plot_keys[0])\n",
    "    plt.plot(x_pred, y_pred, style_pred, label=plot_keys[1])\n",
    "    plt.plot(start_point[0], start_point[1], style_O, label='Start Point')\n",
    "    plt.legend(loc=\"upper right\", prop={'size': fontsize_})\n",
    "    plt.xlabel('x (m)', fontsize=fontsize_)\n",
    "    plt.ylabel('y (m)', fontsize=fontsize_)\n",
    "    # set the range of x and y\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    xmean = np.mean(xlim)\n",
    "    ymean = np.mean(ylim)\n",
    "    plot_radius = max([abs(lim - mean_)\n",
    "                       for lims, mean_ in ((xlim, xmean),\n",
    "                                           (ylim, ymean))\n",
    "                       for lim in lims])\n",
    "    ax.set_xlim([xmean - plot_radius, xmean + plot_radius])\n",
    "    ax.set_ylim([ymean - plot_radius, ymean + plot_radius])\n",
    "\n",
    "    plt.title('2D path')\n",
    "    png_title = \"{}_path_2d_xy\".format(seq)\n",
    "    plt.savefig(plot_path_dir + \"/\" + png_title + \".png\", bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3D trajectory map \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot(x_gt, y_gt, z_gt, style_gt, label=plot_keys[0])\n",
    "    ax.plot(x_pred, y_pred, z_pred, style_pred, label=plot_keys[1])\n",
    "    ax.plot(0, 0, 0, style_O, label='Start Point')\n",
    "    plt.legend(loc=\"upper right\", prop={'size': fontsize_})\n",
    "    plt.xlabel('x (m)', fontsize=fontsize_)\n",
    "    plt.ylabel('y (m)', fontsize=fontsize_)\n",
    "    ax.set_zlabel('z (m)')\n",
    "    \n",
    "    plt.title('3D path')\n",
    "    png_title = \"{}_path_3d\".format(seq)\n",
    "    plt.savefig(plot_path_dir + \"/\" + png_title + \".png\", bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "\n",
    "    # Plot decision hearmap\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=100)\n",
    "    ax = plt.gca()\n",
    "    cout = np.insert(decision, 0, 0) * 100\n",
    "    cax = plt.scatter(x_pred, z_pred, marker='o', c=cout)\n",
    "    plt.xlabel('x (m)', fontsize=fontsize_)\n",
    "    plt.ylabel('z (m)', fontsize=fontsize_)\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    xmean = np.mean(xlim)\n",
    "    ymean = np.mean(ylim)\n",
    "    ax.set_xlim([xmean - plot_radius, xmean + plot_radius])\n",
    "    ax.set_ylim([ymean - plot_radius, ymean + plot_radius])\n",
    "    max_usage = max(cout)\n",
    "    min_usage = min(cout)\n",
    "    ticks = np.floor(np.linspace(min_usage, max_usage, num=5))\n",
    "    cbar = fig.colorbar(cax, ticks=ticks)\n",
    "    cbar.ax.set_yticklabels([str(i) + '%' for i in ticks])\n",
    "\n",
    "    plt.title('decision heatmap with window size {}'.format(window_size))\n",
    "    png_title = \"{}_decision_smoothed\".format(seq)\n",
    "    plt.savefig(plot_path_dir + \"/\" + png_title + \".png\", bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot the speed map\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=100)\n",
    "    ax = plt.gca()\n",
    "    cout = speed\n",
    "    cax = plt.scatter(x_pred, z_pred, marker='o', c=cout)\n",
    "    plt.xlabel('x (m)', fontsize=fontsize_)\n",
    "    plt.ylabel('z (m)', fontsize=fontsize_)\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    xmean = np.mean(xlim)\n",
    "    ymean = np.mean(ylim)\n",
    "    ax.set_xlim([xmean - plot_radius, xmean + plot_radius])\n",
    "    ax.set_ylim([ymean - plot_radius, ymean + plot_radius])\n",
    "    max_speed = max(cout)\n",
    "    min_speed = min(cout)\n",
    "    ticks = np.floor(np.linspace(min_speed, max_speed, num=5))\n",
    "    cbar = fig.colorbar(cax, ticks=ticks)\n",
    "    cbar.ax.set_yticklabels([str(i) + 'm/s' for i in ticks])\n",
    "\n",
    "    plt.title('speed heatmap')\n",
    "    png_title = \"{}_speed\".format(seq)\n",
    "    plt.savefig(plot_path_dir + \"/\" + png_title + \".png\", bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def kitti_err_cal(pose_est_mat, pose_gt_mat):\n",
    "\n",
    "    # metric lengths in meters\n",
    "    \n",
    "    lengths = [100, 200, 300, 400, 500, 600, 700, 800]\n",
    "    # lengths = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    \n",
    "    num_lengths = len(lengths)\n",
    "\n",
    "    err = []\n",
    "    dist, speed = trajectoryDistances(pose_gt_mat)\n",
    "    step_size = 10  # 10Hz\n",
    "\n",
    "    for first_frame in range(0, len(pose_gt_mat), step_size):\n",
    "\n",
    "        calculated_metric_length = 0\n",
    "        \n",
    "        for i in range(num_lengths):\n",
    "            metric_length = lengths[i]\n",
    "            last_frame = lastFrameFromSegmentLength(dist, first_frame, metric_length)\n",
    "            # Continue if sequence not long enough\n",
    "            if last_frame == -1 or last_frame >= len(pose_est_mat) or first_frame >= len(pose_est_mat):\n",
    "                continue\n",
    "            \n",
    "            calculated_metric_length += 1\n",
    "\n",
    "            pose_delta_gt = np.dot(np.linalg.inv(pose_gt_mat[first_frame]), pose_gt_mat[last_frame])\n",
    "            pose_delta_result = np.dot(np.linalg.inv(pose_est_mat[first_frame]), pose_est_mat[last_frame])\n",
    "            \n",
    "            r_err = rotationError(pose_delta_result, pose_delta_gt)\n",
    "            t_err = translationError(pose_delta_result, pose_delta_gt)\n",
    "\n",
    "            err.append([first_frame, r_err / metric_length, t_err / metric_length, metric_length])\n",
    "\n",
    "        # print(\"calculated_metric_length: \", calculated_metric_length)\n",
    "        \n",
    "    t_rel, r_rel = computeOverallErr(err)\n",
    "    \n",
    "    # print(\"t_rel: \", t_rel)\n",
    "    # print(\"r_rel: \", r_rel)\n",
    "    \n",
    "    return err, t_rel, r_rel, np.asarray(speed)\n",
    "\n",
    "def kitti_eval(pose_est, dec_est, pose_gt):\n",
    "    \n",
    "    # First decision is always true\n",
    "    dec_est = np.insert(dec_est, 0, 1)\n",
    "    \n",
    "    # Calculate the translational and rotational RMSE\n",
    "    t_rmse, r_rmse = rmse_err_cal(pose_est, pose_gt)\n",
    "\n",
    "    # Transfer to 3x4 pose matrix\n",
    "    pose_est_mat = path_accu(pose_est)\n",
    "    pose_gt_mat = path_accu(pose_gt)\n",
    "\n",
    "    # Using KITTI metric\n",
    "    err_list, t_rel, r_rel, speed = kitti_err_cal(pose_est_mat, pose_gt_mat)\n",
    "    \n",
    "    \"\"\"\n",
    "    여기 아닌것 같음 - Kitti로도 찍어보자\n",
    "    찍어보니 맞음 - kitti와 nuscenes와의 coord. system 차이에 의해 발생?\n",
    "    \"\"\"\n",
    "    t_rel = t_rel * 100\n",
    "    r_rel = r_rel / np.pi * 180 * 100\n",
    "    r_rmse = r_rmse / np.pi * 180\n",
    "    usage = np.mean(dec_est) * 100\n",
    "\n",
    "    return pose_est_mat, pose_gt_mat, t_rel, r_rel, t_rmse, r_rmse, usage, speed\n",
    "\n",
    "class NuScenes_Tester():\n",
    "    def __init__(self, val_scene_datasets):\n",
    "        super(NuScenes_Tester, self).__init__()\n",
    "        self.val_scene_datasets = val_scene_datasets\n",
    "    \n",
    "    def test_one_scene(self, model, scene_dataset, selection, num_gpu=1, p=0.5):\n",
    "        hc = None\n",
    "        pose_list, decision_list, probs_list, pose_rel_gt_list = [], [], [], []\n",
    "        for i, (image_seq, imu_seq, gt_seq) in tqdm(enumerate(scene_dataset), total=len(scene_dataset), smoothing=0.9):\n",
    "            x_in = image_seq.unsqueeze(0).repeat(num_gpu,1,1,1,1).cuda()\n",
    "            i_in = imu_seq.unsqueeze(0).repeat(num_gpu,1,1).cuda()\n",
    "            with torch.no_grad():\n",
    "                pose, decision, probs, hc = model(x_in, i_in, is_first=(i==0), hc=hc, selection=selection, p=p)\n",
    "            pose_list.append(pose[0,:,:].detach().cpu().numpy())\n",
    "            decision_list.append(decision[0,:,:].detach().cpu().numpy()[:, 0])\n",
    "            probs_list.append(probs[0,:,:].detach().cpu().numpy())\n",
    "            pose_rel_gt_list.append(np.array(gt_seq))\n",
    "        pose_est = np.vstack(pose_list)\n",
    "        dec_est = np.hstack(decision_list)\n",
    "        prob_est = np.vstack(probs_list)\n",
    "        pose_rel_gt_list = np.vstack(pose_rel_gt_list)\n",
    "        return pose_est, dec_est, prob_est, pose_rel_gt_list\n",
    "    \n",
    "    def eval(self, model, selection, num_gpu=1, p=0.5):\n",
    "        self.errors = []\n",
    "        self.est = []\n",
    "\n",
    "        for i, scene_dataset in enumerate(self.val_scene_datasets):\n",
    "            print(f'testing sequence {i}')\n",
    "            \n",
    "            pose_est, dec_est, prob_est, pose_rel_gt_list = self.test_one_scene(model, scene_dataset, selection, num_gpu=num_gpu, p=p)  \n",
    "            pose_est_global, pose_gt_global, t_rel, r_rel, t_rmse, r_rmse, usage, speed = kitti_eval(pose_est, dec_est, pose_rel_gt_list)\n",
    "            \n",
    "            self.est.append({'pose_est_global':pose_est_global, 'pose_gt_global':pose_gt_global, 'decs':dec_est, 'probs':prob_est, 'speed':speed})\n",
    "            self.errors.append({'t_rel':t_rel, 'r_rel':r_rel, 't_rmse':t_rmse, 'r_rmse':r_rmse, 'usage':usage})\n",
    "        \n",
    "        return self.errors\n",
    "    \n",
    "    def generate_plots(self, save_dir, window_size):\n",
    "        for i, scene_dataset in enumerate(self.val_scene_datasets):\n",
    "            plotPath_2D(scene_dataset, \n",
    "                        self.est[i]['pose_gt_global'], \n",
    "                        self.est[i]['pose_est_global'], \n",
    "                        save_dir, \n",
    "                        self.est[i]['decs'], \n",
    "                        self.est[i]['speed'], \n",
    "                        window_size)\n",
    "            \n",
    "    def save_text(self, save_dir):\n",
    "        for i, scene_dataset in enumerate(self.val_scene_datasets):\n",
    "            path = save_dir/'{}_pred.txt'.format(scene_dataset)\n",
    "            saveSequence(self.est[i]['pose_est_global'], path)\n",
    "            print('scene_dataset {} saved'.format(scene_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_partition():\n",
    "    def __init__(self, opt, folder):\n",
    "        super(data_partition, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.data_dir = opt.data_dir\n",
    "        self.seq_len = opt.seq_len\n",
    "        self.folder = folder\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        image_dir = self.data_dir + '/sequences/'\n",
    "        imu_dir = self.data_dir + '/imus/'\n",
    "        pose_dir = self.data_dir + '/poses/'\n",
    "\n",
    "        self.img_paths = glob.glob('{}{}/image_2/*.png'.format(image_dir, self.folder))\n",
    "        self.imus = sio.loadmat('{}{}.mat'.format(imu_dir, self.folder))['imu_data_interp']\n",
    "        self.poses, self.poses_rel = read_pose_from_text('{}{}.txt'.format(pose_dir, self.folder))\n",
    "        self.img_paths.sort()\n",
    "\n",
    "        self.img_paths_list, self.poses_list, self.imus_list = [], [], []\n",
    "        start = 0\n",
    "        n_frames = len(self.img_paths)\n",
    "        while start + self.seq_len < n_frames:\n",
    "            self.img_paths_list.append(self.img_paths[start:start + self.seq_len])\n",
    "            self.poses_list.append(self.poses_rel[start:start + self.seq_len - 1])\n",
    "            self.imus_list.append(self.imus[start * 10:(start + self.seq_len - 1) * 10 + 1])\n",
    "            start += self.seq_len - 1\n",
    "        self.img_paths_list.append(self.img_paths[start:])\n",
    "        self.poses_list.append(self.poses_rel[start:])\n",
    "        self.imus_list.append(self.imus[start * 10:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image_path_sequence = self.img_paths_list[i]\n",
    "        image_sequence = []\n",
    "        for img_path in image_path_sequence:\n",
    "            img_as_img = Image.open(img_path)\n",
    "            img_as_img = TF.resize(img_as_img, size=(self.opt.img_h, self.opt.img_w))\n",
    "            img_as_tensor = TF.to_tensor(img_as_img) - 0.5\n",
    "            img_as_tensor = img_as_tensor.unsqueeze(0)\n",
    "            image_sequence.append(img_as_tensor)\n",
    "        image_sequence = torch.cat(image_sequence, 0)\n",
    "        imu_sequence = torch.FloatTensor(self.imus_list[i])\n",
    "        gt_sequence = self.poses_list[i][:, :6]\n",
    "        return image_sequence, imu_sequence, gt_sequence\n",
    "\n",
    "\n",
    "class KITTI_tester():\n",
    "    def __init__(self, args):\n",
    "        super(KITTI_tester, self).__init__()\n",
    "        \n",
    "        # generate data loader for each path\n",
    "        self.dataloader = []\n",
    "        for seq in args.val_seq:\n",
    "            self.dataloader.append(data_partition(args, seq))\n",
    "\n",
    "        self.args = args\n",
    "    \n",
    "    def test_one_path(self, net, df, selection, num_gpu=1, p=0.5):\n",
    "        hc = None\n",
    "        pose_list, decision_list, probs_list= [], [], []\n",
    "        for i, (image_seq, imu_seq, gt_seq) in tqdm(enumerate(df), total=len(df), smoothing=0.9):  \n",
    "            x_in = image_seq.unsqueeze(0).repeat(num_gpu,1,1,1,1).cuda()\n",
    "            i_in = imu_seq.unsqueeze(0).repeat(num_gpu,1,1).cuda()\n",
    "            with torch.no_grad():\n",
    "                pose, decision, probs, hc = net(x_in, i_in, is_first=(i==0), hc=hc, selection=selection, p=p)\n",
    "            pose_list.append(pose[0,:,:].detach().cpu().numpy())\n",
    "            decision_list.append(decision[0,:,:].detach().cpu().numpy()[:, 0])\n",
    "            probs_list.append(probs[0,:,:].detach().cpu().numpy())\n",
    "        pose_est = np.vstack(pose_list)\n",
    "        dec_est = np.hstack(decision_list)\n",
    "        prob_est = np.vstack(probs_list)        \n",
    "        return pose_est, dec_est, prob_est\n",
    "\n",
    "    def eval(self, net, selection, num_gpu=1, p=0.5):\n",
    "        self.errors = []\n",
    "        self.est = []\n",
    "        for i, seq in enumerate(self.args.val_seq):\n",
    "            print(f'testing sequence {seq}')\n",
    "            pose_est, dec_est, prob_est = self.test_one_path(net, self.dataloader[i], selection, num_gpu=num_gpu, p=p)            \n",
    "            pose_est_global, pose_gt_global, t_rel, r_rel, t_rmse, r_rmse, usage, speed = kitti_eval(pose_est, dec_est, self.dataloader[i].poses_rel)\n",
    "            \n",
    "            self.est.append({'pose_est_global':pose_est_global, 'pose_gt_global':pose_gt_global, 'decs':dec_est, 'probs':prob_est, 'speed':speed})\n",
    "            self.errors.append({'t_rel':t_rel, 'r_rel':r_rel, 't_rmse':t_rmse, 'r_rmse':r_rmse, 'usage':usage})\n",
    "            \n",
    "        return self.errors\n",
    "\n",
    "    def generate_plots(self, save_dir, window_size):\n",
    "        for i, seq in enumerate(self.args.val_seq):\n",
    "            plotPath_2D(seq, \n",
    "                        self.est[i]['pose_gt_global'], \n",
    "                        self.est[i]['pose_est_global'], \n",
    "                        save_dir, \n",
    "                        self.est[i]['decs'], \n",
    "                        self.est[i]['speed'], \n",
    "                        window_size)\n",
    "    \n",
    "    def save_text(self, save_dir):\n",
    "        for i, seq in enumerate(self.args.val_seq):\n",
    "            path = save_dir/'{}_pred.txt'.format(seq)\n",
    "            saveSequence(self.est[i]['pose_est_global'], path)\n",
    "            print('Seq {} saved'.format(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'hflip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/data/home/jeholee/omni3D/deepvio-nuscenes/nuscenes_vio.ipynb Cell 10\u001b[0m line \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.140.76/data/home/jeholee/omni3D/deepvio-nuscenes/nuscenes_vio.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Load the dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.140.76/data/home/jeholee/omni3D/deepvio-nuscenes/nuscenes_vio.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m transform_train \u001b[39m=\u001b[39m [custom_transform\u001b[39m.\u001b[39mToTensor(), custom_transform\u001b[39m.\u001b[39mResize((args\u001b[39m.\u001b[39mimg_h, args\u001b[39m.\u001b[39mimg_w))]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B165.132.140.76/data/home/jeholee/omni3D/deepvio-nuscenes/nuscenes_vio.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39;49mhflip:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.140.76/data/home/jeholee/omni3D/deepvio-nuscenes/nuscenes_vio.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     transform_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [custom_transform\u001b[39m.\u001b[39mRandomHorizontalFlip()]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.140.76/data/home/jeholee/omni3D/deepvio-nuscenes/nuscenes_vio.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mcolor:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'hflip'"
     ]
    }
   ],
   "source": [
    "mmcv.mkdir_or_exist(args.save_dir)\n",
    "checkpoints_dir = os.path.join(args.save_dir, \"experiment_1\")\n",
    "mmcv.mkdir_or_exist(checkpoints_dir)\n",
    "\n",
    "# Create logs\n",
    "logger = logging.getLogger(args.experiment_name)\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger.info('----------------------------------------TRAINING----------------------------------')\n",
    "logger.info('PARAMETER ...')\n",
    "logger.info(args)\n",
    "\n",
    "# Load the dataset\n",
    "transform_train = [custom_transform.ToTensor(), custom_transform.Resize((args.img_h, args.img_w))]\n",
    "if args.hflip:\n",
    "    transform_train += [custom_transform.RandomHorizontalFlip()]\n",
    "if args.color:\n",
    "    transform_train += [custom_transform.RandomColorAug()]\n",
    "transform_train = custom_transform.Compose(transform_train)\n",
    "\n",
    "##############################################################\n",
    "max_imu_length = 11 # KITTI\n",
    "##############################################################\n",
    "\n",
    "val_dataset = NuScenes_Dataset(dataroot,\n",
    "                                 mode='val',\n",
    "                             sequence_length=args.seq_len,\n",
    "                             max_imu_length=max_imu_length,\n",
    "                             cam_names=cam_names,\n",
    "                             transform=transform_train,\n",
    "                             nusc=nusc,\n",
    "                             nusc_can=nusc_can,\n",
    "                             args=args)\n",
    "\n",
    "val_scene_datasets = val_dataset.get_val_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model ./pretrained_models/vf_512_if_256_3e-05.model\n",
      "testing sequence 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:10<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_rel:  1.4327209127773595\n",
      "r_rel:  0.004775360509008873\n",
      "testing sequence 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:08<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_rel:  1.7356491427690386\n",
      "r_rel:  0.005148786236864521\n",
      "testing sequence 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:09<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_rel:  1.4624186933996497\n",
      "r_rel:  0.004441598131854362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GPU selections\n",
    "str_ids = device.split(',')\n",
    "gpu_ids = []\n",
    "for str_id in str_ids:\n",
    "    id = int(str_id)\n",
    "    if id >= 0:\n",
    "        gpu_ids.append(id)\n",
    "if len(gpu_ids) > 0:\n",
    "    torch.cuda.set_device(gpu_ids[0])\n",
    "\n",
    "# Initialize the tester\n",
    "tester = NuScenes_Tester(val_scene_datasets)\n",
    "\n",
    "# Model initialization\n",
    "model = DeepVIO(args)\n",
    "\n",
    "ckpt_path = './pretrained_models/vf_512_if_256_3e-05.model'\n",
    "model.load_state_dict(torch.load(ckpt_path))\n",
    "print('load model %s'%ckpt_path)\n",
    "\n",
    "# Feed model to GPU\n",
    "model.cuda(gpu_ids[0])\n",
    "model = torch.nn.DataParallel(model, device_ids = gpu_ids)\n",
    "\n",
    "model.eval()\n",
    "errors = tester.eval(model, 'gumbel-softmax', num_gpu=len(gpu_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.generate_plots('./results/experiment_1/nuscenes/', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--data_dir', type=str, default='./data/kitti/', help='path to the dataset')\n",
    "parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
    "parser.add_argument('--save_dir', type=str, default='./results', help='path to save the result')\n",
    "parser.add_argument('--seq_len', type=int, default=11, help='sequence length for LSTM')\n",
    "\n",
    "parser.add_argument('--train_seq', type=list, default=['00', '01', '02', '04', '06', '08', '09'], help='sequences for training')\n",
    "parser.add_argument('--val_seq', type=list, default=['07'], help='sequences for validation')\n",
    "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
    "\n",
    "parser.add_argument('--img_w', type=int, default=512, help='image width')\n",
    "parser.add_argument('--img_h', type=int, default=256, help='image height')\n",
    "parser.add_argument('--v_f_len', type=int, default=512, help='visual feature length')\n",
    "parser.add_argument('--i_f_len', type=int, default=256, help='imu feature length')\n",
    "parser.add_argument('--fuse_method', type=str, default='cat', help='fusion method [cat, soft, hard]')\n",
    "parser.add_argument('--imu_dropout', type=float, default=0, help='dropout for the IMU encoder')\n",
    "\n",
    "parser.add_argument('--rnn_hidden_size', type=int, default=1024, help='size of the LSTM latent')\n",
    "parser.add_argument('--rnn_dropout_out', type=float, default=0.2, help='dropout for the LSTM output layer')\n",
    "parser.add_argument('--rnn_dropout_between', type=float, default=0.2, help='dropout within LSTM')\n",
    "\n",
    "parser.add_argument('--workers', type=int, default=4, help='number of workers')\n",
    "parser.add_argument('--experiment_name', type=str, default='test', help='experiment name')\n",
    "parser.add_argument('--model', type=str, default='./pretrain_models/vf_512_if_256_3e-05.model', help='path to the pretrained model')\n",
    "args = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialize the tester\n",
    "tester = KITTI_tester(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing sequence 07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:24<00:00,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_rel:  0.017259536462358335\n",
      "r_rel:  0.00012587293560097494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "errors = tester.eval(model, 'gumbel-softmax', num_gpu=len(gpu_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'t_rel': 143.27209127773594,\n",
       "  'r_rel': 27.360800281965293,\n",
       "  't_rmse': 1.1589466929184404,\n",
       "  'r_rmse': 0.7952650806784035,\n",
       "  'usage': 18.26086938381195},\n",
       " {'t_rel': 173.56491427690386,\n",
       "  'r_rel': 29.500372098738243,\n",
       "  't_rmse': 1.1845246747014186,\n",
       "  'r_rmse': 0.7671784817745677,\n",
       "  'usage': 32.72727131843567},\n",
       " {'t_rel': 146.24186933996498,\n",
       "  'r_rel': 25.448482724844588,\n",
       "  't_rmse': 1.1813577173057124,\n",
       "  'r_rmse': 0.8037528325557416,\n",
       "  'usage': 17.72727221250534}]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.generate_plots(\"./results/experiment_1/\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU selections\n",
    "str_ids = device.split(',')\n",
    "gpu_ids = []\n",
    "for str_id in str_ids:\n",
    "    id = int(str_id)\n",
    "    if id >= 0:\n",
    "        gpu_ids.append(id)\n",
    "if len(gpu_ids) > 0:\n",
    "    torch.cuda.set_device(gpu_ids[0])\n",
    "\n",
    "# Initialize the tester\n",
    "tester = KITTI_tester(args)\n",
    "    \n",
    "# Model initialization\n",
    "model = DeepVIO(args)\n",
    "\n",
    "# Continual training or not\n",
    "if args.pretrain is not None:\n",
    "    model.load_state_dict(torch.load(args.pretrain))\n",
    "    print('load model %s'%args.pretrain)\n",
    "    logger.info('load model %s'%args.pretrain)\n",
    "else:\n",
    "    print('Training from scratch')\n",
    "    logger.info('Training from scratch')\n",
    "\n",
    "# Use the pre-trained flownet or not\n",
    "if args.pretrain_flownet and args.pretrain is None:\n",
    "    pretrained_w = torch.load(args.pretrain_flownet, map_location='cpu')\n",
    "    model_dict = model.Feature_net.state_dict()\n",
    "    update_dict = {k: v for k, v in pretrained_w['state_dict'].items() if k in model_dict}\n",
    "    model_dict.update(update_dict)\n",
    "    model.Feature_net.load_state_dict(model_dict)\n",
    "\n",
    "# Feed model to GPU\n",
    "# model.to(device)\n",
    "# model = torch.nn.DataParallel(model, device_ids = [device])\n",
    "\n",
    "# model = model.cuda()\n",
    "model.cuda(gpu_ids[0])\n",
    "model = torch.nn.DataParallel(model, device_ids = gpu_ids)\n",
    "\n",
    "pretrain = args.pretrain \n",
    "if args.pretrain is None or pretrain[-5:] == 'model':\n",
    "    init_epoch = 0\n",
    "else:\n",
    "    init_epoch = int(pretrain[-7:-4])+1\n",
    "\n",
    "# Initialize the optimizer\n",
    "if args.optimizer == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "elif args.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, weight_decay=args.weight_decay)\n",
    "\n",
    "best = 10000\n",
    "\n",
    "for ep in range(init_epoch, args.epochs_warmup+args.epochs_joint+args.epochs_fine):\n",
    "    lr, selection, temp = update_status(ep, args, model)\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    message = f'Epoch: {ep}, lr: {lr}, selection: {selection}, temperaure: {temp:.5f}'\n",
    "    print(message)\n",
    "    logger.info(message)\n",
    "    \n",
    "    model.train()\n",
    "    avg_pose_loss, avg_penalty_loss = train(model, optimizer, train_loader, selection, temp, logger, ep, p=0.5)\n",
    "    \n",
    "    if ep > args.epochs_warmup+args.epochs_joint:\n",
    "        # Save the model after training\n",
    "        torch.save(model.module.state_dict(), f'{checkpoints_dir}/{ep:003}.pth')\n",
    "        message = f'Epoch {ep} training finished, pose loss: {avg_pose_loss:.6f}, penalty_loss: {avg_penalty_loss:.6f}, model saved'\n",
    "        print(message)\n",
    "        logger.info(message)\n",
    "    \n",
    "        # Evaluate the model\n",
    "        # print('Evaluating the model')\n",
    "        # logger.info('Evaluating the model')\n",
    "        # with torch.no_grad(): \n",
    "        #     model.eval()\n",
    "        #     errors = tester.eval(model, selection='gumbel-softmax', num_gpu=len(gpu_ids))\n",
    "    \n",
    "        # t_rel = np.mean([errors[i]['t_rel'] for i in range(len(errors))])\n",
    "        # r_rel = np.mean([errors[i]['r_rel'] for i in range(len(errors))])\n",
    "        # t_rmse = np.mean([errors[i]['t_rmse'] for i in range(len(errors))])\n",
    "        # r_rmse = np.mean([errors[i]['r_rmse'] for i in range(len(errors))])\n",
    "        # usage = np.mean([errors[i]['usage'] for i in range(len(errors))])\n",
    "\n",
    "        # if t_rel < best:\n",
    "        #     best = t_rel \n",
    "        #     torch.save(model.module.state_dict(), f'{checkpoints_dir}/best_{best:.2f}.pth')\n",
    "    \n",
    "        # message = f'Epoch {ep} evaluation finished , t_rel: {t_rel:.4f}, r_rel: {r_rel:.4f}, t_rmse: {t_rmse:.4f}, r_rmse: {r_rmse:.4f}, usage: {usage:.4f}, best t_rel: {best:.4f}'\n",
    "        # logger.info(message)\n",
    "        # print(message)\n",
    "\n",
    "message = f'Training finished, best t_rel: {best:.4f}'\n",
    "logger.info(message)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
