{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.can_bus.can_bus_api import NuScenesCanBus\n",
    "from nuscenes.utils import splits\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import pprint\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from path import Path\n",
    "from utils import custom_transform\n",
    "from dataset.KITTI_dataset import KITTI\n",
    "from model import DeepVIO\n",
    "from collections import defaultdict\n",
    "from utils.kitti_eval import KITTI_tester, data_partition\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from utils.utils import *\n",
    "\n",
    "from utils.utils import rotationError, read_pose_from_text\n",
    "from collections import Counter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "from scipy.ndimage import convolve1d\n",
    "from torch.utils.data import Dataset\n",
    "from utils import custom_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_rotation_matrix(Q):\n",
    "    \"\"\"\n",
    "    Covert a quaternion into a full three-dimensional rotation matrix.\n",
    " \n",
    "    Input\n",
    "    :param Q: A 4 element array representing the quaternion (q0,q1,q2,q3) \n",
    " \n",
    "    Output\n",
    "    :return: A 3x3 element matrix representing the full 3D rotation matrix. \n",
    "             This rotation matrix converts a point in the local reference \n",
    "             frame to a point in the global reference frame.\n",
    "    \"\"\"\n",
    "    # Extract the values from Q\n",
    "    q0 = Q[0]\n",
    "    q1 = Q[1]\n",
    "    q2 = Q[2]\n",
    "    q3 = Q[3]\n",
    "     \n",
    "    # First row of the rotation matrix\n",
    "    r00 = 2 * (q0 * q0 + q1 * q1) - 1\n",
    "    r01 = 2 * (q1 * q2 - q0 * q3)\n",
    "    r02 = 2 * (q1 * q3 + q0 * q2)\n",
    "     \n",
    "    # Second row of the rotation matrix\n",
    "    r10 = 2 * (q1 * q2 + q0 * q3)\n",
    "    r11 = 2 * (q0 * q0 + q2 * q2) - 1\n",
    "    r12 = 2 * (q2 * q3 - q0 * q1)\n",
    "     \n",
    "    # Third row of the rotation matrix\n",
    "    r20 = 2 * (q1 * q3 - q0 * q2)\n",
    "    r21 = 2 * (q2 * q3 + q0 * q1)\n",
    "    r22 = 2 * (q0 * q0 + q3 * q3) - 1\n",
    "     \n",
    "    # 3x3 rotation matrix\n",
    "    rot_matrix = np.array([[r00, r01, r02],\n",
    "                           [r10, r11, r12],\n",
    "                           [r20, r21, r22]])\n",
    "                            \n",
    "    return rot_matrix\n",
    "\n",
    "\n",
    "def euler_from_matrix(matrix):\n",
    "    '''\n",
    "    Extract the eular angle from a rotation matrix\n",
    "    '''\n",
    "    _EPS = np.finfo(float).eps * 4.0\n",
    "    \n",
    "    M = np.array(matrix, dtype=np.float64, copy=False)[:3, :3]\n",
    "    cy = math.sqrt(M[0, 0] * M[0, 0] + M[1, 0] * M[1, 0])\n",
    "    ay = math.atan2(-M[2, 0], cy)\n",
    "    if ay < -math.pi / 2 + _EPS and ay > -math.pi / 2 - _EPS:  # pitch = -90 deg\n",
    "        ax = 0\n",
    "        az = math.atan2(-M[1, 2], -M[0, 2])\n",
    "    elif ay < math.pi / 2 + _EPS and ay > math.pi / 2 - _EPS:\n",
    "        ax = 0\n",
    "        az = math.atan2(M[1, 2], M[0, 2])\n",
    "    else:\n",
    "        ax = math.atan2(M[2, 1], M[2, 2])\n",
    "        az = math.atan2(M[1, 0], M[0, 0])\n",
    "    return np.array([ax, ay, az])\n",
    "\n",
    "def get_lds_kernel_window(kernel, ks, sigma):\n",
    "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "    half_ks = (ks - 1) // 2\n",
    "    if kernel == 'gaussian':\n",
    "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "    elif kernel == 'triang':\n",
    "        kernel_window = triang(ks)\n",
    "    else:\n",
    "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "    return kernel_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "dataroot = '/data/public/360_3D_OD_Dataset/nuscenes'\n",
    "cam_names = [\"CAM_FRONT\", \"CAM_FRONT_RIGHT\", \"CAM_BACK_RIGHT\", \"CAM_BACK\", \"CAM_BACK_LEFT\", \"CAM_FRONT_LEFT\"]\n",
    "#########################################################################################\n",
    "\n",
    "nusc_can = NuScenesCanBus(dataroot=dataroot)\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot=dataroot, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3,4\"  # Set the GPUs 2 and 3 to use\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = '3, 4'\n",
    "# device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nusc_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_root,\n",
    "                 sequence_length=11,\n",
    "                 max_imu_length=10,\n",
    "                 cam_names = [\"CAM_FRONT\", \"CAM_FRONT_RIGHT\", \"CAM_BACK_RIGHT\", \"CAM_BACK\", \"CAM_BACK_LEFT\", \"CAM_FRONT_LEFT\"],\n",
    "                 transform=None,\n",
    "                 nusc=None,\n",
    "                 nusc_can=None):\n",
    "        self.data_root = data_root\n",
    "        if nusc is None:\n",
    "            self.nusc = NuScenes(version='v1.0-trainval', dataroot=self.data_root, verbose=False)\n",
    "        else:\n",
    "            self.nusc = nusc\n",
    "        if nusc_can is None:\n",
    "            self.nusc_can = NuScenesCanBus(dataroot=self.data_root)\n",
    "        else:\n",
    "            self.nusc_can = nusc_can\n",
    "        self.cam_names = cam_names\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_imu_length = max_imu_length\n",
    "        self.transform = transform\n",
    "        self.make_dataset()\n",
    "    \n",
    "    def get_available_scene_tokens(self):\n",
    "        \"\"\"Code from bevdet codebase - tools/data_converter/nuscenes_converter.py\"\"\"\n",
    "        train_scenes = splits.train\n",
    "        val_scenes = splits.val\n",
    "\n",
    "        available_scenes = []\n",
    "        for scene in self.nusc.scene:\n",
    "            scene_token = scene['token']\n",
    "            scene_rec = self.nusc.get('scene', scene_token)\n",
    "            sample_rec = self.nusc.get('sample', scene_rec['first_sample_token'])\n",
    "            sd_rec = self.nusc.get('sample_data', sample_rec['data']['LIDAR_TOP'])\n",
    "            has_more_frames = True\n",
    "            scene_not_exist = False\n",
    "            while has_more_frames:\n",
    "                lidar_path, boxes, _ = self.nusc.get_sample_data(sd_rec['token'])\n",
    "                lidar_path = str(lidar_path)\n",
    "                if os.getcwd() in lidar_path:\n",
    "                    # path from lyftdataset is absolute path\n",
    "                    lidar_path = lidar_path.split(f'{os.getcwd()}/')[-1]\n",
    "                    # relative path\n",
    "                if not mmcv.is_filepath(lidar_path):\n",
    "                    scene_not_exist = True\n",
    "                    break\n",
    "                else:\n",
    "                    break\n",
    "            if scene_not_exist:\n",
    "                continue\n",
    "            available_scenes.append(scene)\n",
    "\n",
    "        available_scene_names = [s['name'] for s in available_scenes]\n",
    "        train_scenes = list(filter(lambda x: x in available_scene_names, train_scenes))\n",
    "        val_scenes = list(filter(lambda x: x in available_scene_names, val_scenes))\n",
    "        \n",
    "        train_scenes = set([\n",
    "            available_scenes[available_scene_names.index(s)]['token']\n",
    "            for s in train_scenes\n",
    "        ])\n",
    "        val_scenes = set([\n",
    "            available_scenes[available_scene_names.index(s)]['token']\n",
    "            for s in val_scenes\n",
    "        ])\n",
    "        \n",
    "        train_scenes = [self.nusc.get('scene', token) for token in train_scenes]\n",
    "        val_scenes = [self.nusc.get('scene', token) for token in val_scenes]\n",
    "        \n",
    "        return train_scenes, val_scenes\n",
    "    \n",
    "    def get_scene_data(self, scene_record, cam_name):\n",
    "        scene_name = scene_record['name']\n",
    "\n",
    "        # Get images and poses of target scene\n",
    "        first_sample_token = scene_record['first_sample_token']\n",
    "        cur_sample = self.nusc.get('sample', first_sample_token)\n",
    "        cur_sample_data = self.nusc.get('sample_data', cur_sample['data'][cam_name])\n",
    "\n",
    "        scene_sample_data = []\n",
    "        while True:\n",
    "            try:\n",
    "                scene_sample_data.append(cur_sample_data)\n",
    "                cur_sample_data = self.nusc.get('sample_data', cur_sample_data['next'])\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        scene_imu_data = self.nusc_can.get_messages(scene_name, 'ms_imu')\n",
    "        \n",
    "        return scene_sample_data, scene_imu_data\n",
    "    \n",
    "    def format_training_inputs(self, scene_sample_data, scene_imu_data):\n",
    "        \"\"\" Collect image (12hz), pose (12hz), imu data (96hz) of target scene - single training input contains 2 images,  \"\"\"\n",
    "        # 1. 일단 각 training input 모으기 - 2 images, 2 pose, 1 relative pose, 8 imu data\n",
    "        training_inputs = []\n",
    "        for data_idx, cur_sample_data in enumerate(scene_sample_data):\n",
    "            \n",
    "            # 1. get image \n",
    "            cur_img_path = os.path.join(self.data_root, cur_sample_data['filename'])\n",
    "            if cur_sample_data['next'] != \"\":\n",
    "                next_sample_data = self.nusc.get('sample_data', cur_sample_data['next'])\n",
    "                next_img_path = os.path.join(self.data_root, next_sample_data['filename'])\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "            # 2. get ego pose\n",
    "            # read_pose in utils.py\n",
    "            cur_ego_pose = self.nusc.get('ego_pose', cur_sample_data['ego_pose_token'])\n",
    "            trans = np.array(cur_ego_pose['translation'])\n",
    "            trans = trans.reshape(3, -1)\n",
    "            rot_mat = quaternion_rotation_matrix(cur_ego_pose['rotation']) # (w, x, y, z)\n",
    "            cur_ego_pose_mat = np.concatenate((rot_mat, trans), axis=1)\n",
    "            cur_ego_pose_mat = np.array(cur_ego_pose_mat).reshape(3, 4)\n",
    "            cur_ego_pose_mat = np.concatenate((cur_ego_pose_mat, np.array([[0, 0, 0, 1]])), 0)\n",
    "            \n",
    "            next_ego_pose = self.nusc.get('ego_pose', next_sample_data['ego_pose_token'])\n",
    "            trans = np.array(next_ego_pose['translation'])\n",
    "            trans = trans.reshape(3, -1)\n",
    "            rot_mat = quaternion_rotation_matrix(next_ego_pose['rotation']) # (w, x, y, z)\n",
    "            next_ego_pose_mat = np.concatenate((rot_mat, trans), axis=1)\n",
    "            next_ego_pose_mat = np.array(next_ego_pose_mat).reshape(3, 4)\n",
    "            next_ego_pose_mat = np.concatenate((next_ego_pose_mat, np.array([[0, 0, 0, 1]])), 0)    \n",
    "\n",
    "            # 3. get relative pose\n",
    "            relative_pose = np.dot(np.linalg.inv(cur_ego_pose_mat), next_ego_pose_mat)\n",
    "            R_rel = relative_pose[:3, :3]\n",
    "            t_rel = relative_pose[:3, 3]\n",
    "\n",
    "                # Extract the Eular angle from the relative rotation matrix\n",
    "            x, y, z = euler_from_matrix(R_rel)\n",
    "            theta = [x, y, z]\n",
    "\n",
    "            pose_rel = np.concatenate((theta, t_rel))\n",
    "            \n",
    "            # 4. get imu data\n",
    "            cur_timestamp = cur_sample_data['timestamp']\n",
    "            next_timestamp = next_sample_data['timestamp']\n",
    "            \n",
    "            # get imu data between cur and next timestamp\n",
    "            imu_data = []\n",
    "            for imu in scene_imu_data:\n",
    "                imu_timestamp = imu['utime']\n",
    "                if imu_timestamp > cur_timestamp and imu_timestamp < next_timestamp:\n",
    "                    data = imu['linear_accel'] + imu['rotation_rate']\n",
    "                    imu_data.append(data)\n",
    "            \n",
    "            # if no matched imu data, skip\n",
    "            if len(imu_data) <= 2:\n",
    "                # continue\n",
    "                return None\n",
    "                \n",
    "            # if imu data length is less than max_imu_length, pad with zeros\n",
    "            if len(imu_data) < self.max_imu_length:\n",
    "                imu_data = np.pad(imu_data, ((0, self.max_imu_length - len(imu_data)), (0, 0)), 'constant', constant_values=0)\n",
    "            else:\n",
    "                imu_data = imu_data[:self.max_imu_length]\n",
    "            \n",
    "            # 5. make training input\n",
    "            training_input = {\n",
    "                'cur_img_path': cur_img_path,\n",
    "                'next_img_path': next_img_path,\n",
    "                'cur_ego_pose': cur_ego_pose_mat,\n",
    "                'next_ego_pose': next_ego_pose_mat,\n",
    "                'pose_rel': pose_rel,\n",
    "                'imu_data': imu_data\n",
    "            }\n",
    "            training_inputs.append(training_input)\n",
    "        return training_inputs\n",
    "    \n",
    "    def segment_training_inputs(self, training_inputs):\n",
    "        samples = []\n",
    "\n",
    "        input_idx = 0\n",
    "        while True:\n",
    "            # get training input chunk of sequence_length\n",
    "            training_input_chunk = training_inputs[input_idx : input_idx + (self.sequence_length-1)]\n",
    "            \n",
    "            input_idx += 1 # training sequence간 겹치는 images 존재함\n",
    "            \n",
    "            if len(training_input_chunk) < (self.sequence_length-1):\n",
    "                break\n",
    "            \n",
    "            img_samples = []\n",
    "            pose_samples = []\n",
    "            for training_input in training_input_chunk:\n",
    "                img_samples.append(training_input['cur_img_path'])\n",
    "                pose_samples.append(training_input['cur_ego_pose'])\n",
    "            img_samples.append(training_input_chunk[-1]['next_img_path'])\n",
    "            pose_samples.append(training_input_chunk[-1]['next_ego_pose'])\n",
    "            \n",
    "            pose_rel_samples = []\n",
    "            imu_samples = np.empty((0, 6))\n",
    "            for training_input in training_input_chunk:\n",
    "                pose_rel_samples.append(training_input['pose_rel'])\n",
    "                imu_samples = np.vstack((imu_samples, np.array(training_input['imu_data'])))\n",
    "            \n",
    "            pose_samples = np.array(pose_samples)\n",
    "            pose_rel_samples = np.array(pose_rel_samples)\n",
    "            imu_samples = np.array(imu_samples)\n",
    "    \n",
    "            segment_rot = rotationError(pose_samples[0], pose_samples[-1])\n",
    "            sample = {'imgs':img_samples, 'imus':imu_samples, 'gts': pose_rel_samples, 'rot': segment_rot}\n",
    "            \n",
    "            samples.append(sample)\n",
    "            \n",
    "        # Generate weights based on the rotation of the training segments\n",
    "        # Weights are calculated based on the histogram of rotations according to the method in https://github.com/YyzHarry/imbalanced-regression\n",
    "        rot_list = np.array([np.cbrt(item['rot']*180/np.pi) for item in samples])\n",
    "        rot_range = np.linspace(np.min(rot_list), np.max(rot_list), num=10)\n",
    "        indexes = np.digitize(rot_list, rot_range, right=False)\n",
    "        num_samples_of_bins = dict(Counter(indexes))\n",
    "        emp_label_dist = [num_samples_of_bins.get(i, 0) for i in range(1, len(rot_range)+1)]\n",
    "\n",
    "        # Apply 1d convolution to get the smoothed effective label distribution\n",
    "        lds_kernel_window = get_lds_kernel_window(kernel='gaussian', ks=7, sigma=5)\n",
    "        eff_label_dist = convolve1d(np.array(emp_label_dist), weights=lds_kernel_window, mode='constant')\n",
    "\n",
    "        weights = [np.float32(1/eff_label_dist[bin_idx-1]) for bin_idx in indexes]\n",
    "        \n",
    "        assert len(samples) == len(weights)\n",
    "        \n",
    "        return samples, weights\n",
    "    \n",
    "    def make_dataset(self):\n",
    "        train_scenes, val_scenes = self.get_available_scene_tokens()\n",
    "        self.samples, self.weights = [], []\n",
    "\n",
    "        skipped_scene = []\n",
    "        imuavail_train_scenes = []\n",
    "        for idx, train_scene in enumerate(train_scenes):\n",
    "            scene_name = train_scene['name']\n",
    "            scene_idx = int(scene_name.split('-')[-1])\n",
    "            if scene_idx in self.nusc_can.route_blacklist or scene_idx in self.nusc_can.can_blacklist: # skip if scene has no can_bus data\n",
    "                skipped_scene.append(scene_name)\n",
    "                continue\n",
    "            imuavail_train_scenes.append(train_scene)\n",
    "        \n",
    "        target_train_scenes = []\n",
    "        for idx, train_scene in enumerate(imuavail_train_scenes):\n",
    "            avail_cam_num = 0\n",
    "            for cam_name in self.cam_names:\n",
    "                scene_sample_data, scene_imu_data = self.get_scene_data(train_scene, cam_name)\n",
    "                scene_training_inputs = self.format_training_inputs(scene_sample_data, scene_imu_data)\n",
    "                if scene_training_inputs is None: # skip if there are any scene samples that have no associated imu data\n",
    "                    break\n",
    "                avail_cam_num += 1\n",
    "            if avail_cam_num == len(self.cam_names):\n",
    "                target_train_scenes.append(train_scene)\n",
    "            else:\n",
    "                skipped_scene.append(train_scene['name'])\n",
    "        \n",
    "        for idx, train_scene in enumerate(target_train_scenes):\n",
    "            \n",
    "            # select camera one by one\n",
    "            cam_name = self.cam_names[idx % len(self.cam_names)]\n",
    "            \n",
    "            # collect samples and weights                \n",
    "            scene_sample_data, scene_imu_data = self.get_scene_data(train_scene, cam_name)\n",
    "            scene_training_inputs = self.format_training_inputs(scene_sample_data, scene_imu_data)\n",
    "            scene_samples, scene_weights = self.segment_training_inputs(scene_training_inputs)\n",
    "            self.samples.extend(scene_samples)\n",
    "            self.weights.extend(scene_weights)\n",
    "        \n",
    "        print('skipped scenes: {}'.format(len(skipped_scene)))\n",
    "        print('total samples: {}'.format(len(self.samples)))\n",
    "        assert len(self.samples) == len(self.weights)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        imgs = [np.asarray(Image.open(img)) for img in sample['imgs']]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            # imgs, imus, gts = self.transform(imgs, np.copy(sample['imus']), np.copy(sample['gts']))\n",
    "            imgs, imus, gts = self.transform(imgs, np.copy(sample['imus']).astype(np.float32), np.copy(sample['gts']).astype(np.float32))\n",
    "        else:\n",
    "            imus = np.copy(sample['imus'])\n",
    "            gts = np.copy(sample['gts']).astype(np.float32)\n",
    "        \n",
    "        rot = sample['rot'].astype(np.float32)\n",
    "        weight = self.weights[index]\n",
    "\n",
    "        return imgs, imus, gts, rot, weight\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--save_dir', type=str, default='./results', help='path to save the result')\n",
    "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
    "\n",
    "# jeho\n",
    "# parser.add_argument('--img_w', type=int, default=512, help='image width')\n",
    "# parser.add_argument('--img_h', type=int, default=256, help='image height')\n",
    "parser.add_argument('--img_w', type=int, default=448, help='image width')\n",
    "parser.add_argument('--img_h', type=int, default=256, help='image height')\n",
    "\n",
    "parser.add_argument('--v_f_len', type=int, default=512, help='visual feature length')\n",
    "parser.add_argument('--i_f_len', type=int, default=256, help='imu feature length')\n",
    "parser.add_argument('--fuse_method', type=str, default='cat', help='fusion method [cat, soft, hard]')\n",
    "parser.add_argument('--imu_dropout', type=float, default=0, help='dropout for the IMU encoder')\n",
    "parser.add_argument('--rnn_hidden_size', type=int, default=1024, help='size of the LSTM latent')\n",
    "parser.add_argument('--rnn_dropout_out', type=float, default=0.2, help='dropout for the LSTM output layer')\n",
    "parser.add_argument('--rnn_dropout_between', type=float, default=0.2, help='dropout within LSTM')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-6, help='weight decay for the optimizer')\n",
    "\n",
    "parser.add_argument('--seq_len', type=int, default=11, help='sequence length for LSTM')\n",
    "parser.add_argument('--workers', type=int, default=4, help='number of workers')\n",
    "\n",
    "# jeho\n",
    "# NuScenes - 68,000 training samples, total 25 epochs -> 1,700,000 iterations assuming batch size 1\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--epochs_warmup', type=int, default=5, help='number of epochs for warmup')\n",
    "parser.add_argument('--epochs_joint', type=int, default=15, help='number of epochs for joint training')\n",
    "parser.add_argument('--epochs_fine', type=int, default=5, help='number of epochs for finetuning')\n",
    "\n",
    "# KITTI - 17,000 training samples, total 100 epochs -> 1,700,000 iterations assuming batch size 1\n",
    "# parser.add_argument('--epochs_warmup', type=int, default=40, help='number of epochs for warmup')\n",
    "# parser.add_argument('--epochs_joint', type=int, default=40, help='number of epochs for joint training')\n",
    "# parser.add_argument('--epochs_fine', type=int, default=20, help='number of epochs for finetuning')\n",
    "\n",
    "\n",
    "parser.add_argument('--lr_warmup', type=float, default=5e-4, help='learning rate for warming up stage')\n",
    "parser.add_argument('--lr_joint', type=float, default=5e-5, help='learning rate for joint training stage')\n",
    "parser.add_argument('--lr_fine', type=float, default=1e-6, help='learning rate for finetuning stage')\n",
    "parser.add_argument('--eta', type=float, default=0.05, help='exponential decay factor for temperature')\n",
    "parser.add_argument('--temp_init', type=float, default=5, help='initial temperature for gumbel-softmax')\n",
    "parser.add_argument('--Lambda', type=float, default=3e-5, help='penalty factor for the visual encoder usage')\n",
    "\n",
    "parser.add_argument('--experiment_name', type=str, default='experiment', help='experiment name')\n",
    "parser.add_argument('--optimizer', type=str, default='Adam', help='type of optimizer [Adam, SGD]')\n",
    "\n",
    "parser.add_argument('--pretrain_flownet',type=str, default='./pretrained_models/flownets_bn_EPE2.459.pth.tar', help='wehther to use the pre-trained flownet')\n",
    "parser.add_argument('--pretrain', type=str, default=None, help='path to the pretrained model')\n",
    "parser.add_argument('--hflip', default=False, action='store_true', help='whether to use horizonal flipping as augmentation')\n",
    "parser.add_argument('--color', default=False, action='store_true', help='whether to use color augmentations')\n",
    "\n",
    "parser.add_argument('--print_frequency', type=int, default=10, help='print frequency for loss values')\n",
    "parser.add_argument('--weighted', default=False, action='store_true', help='whether to use weighted sum')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "def update_status(ep, args, model):\n",
    "    if ep < args.epochs_warmup:  # Warmup stage\n",
    "        lr = args.lr_warmup\n",
    "        selection = 'random'\n",
    "        temp = args.temp_init\n",
    "        for param in model.module.Policy_net.parameters(): # Disable the policy network\n",
    "            param.requires_grad = False\n",
    "    elif ep >= args.epochs_warmup and ep < args.epochs_warmup + args.epochs_joint: # Joint training stage\n",
    "        lr = args.lr_joint\n",
    "        selection = 'gumbel-softmax'\n",
    "        temp = args.temp_init * math.exp(-args.eta * (ep-args.epochs_warmup))\n",
    "        for param in model.module.Policy_net.parameters(): # Enable the policy network\n",
    "            param.requires_grad = True\n",
    "    elif ep >= args.epochs_warmup + args.epochs_joint: # Finetuning stage\n",
    "        lr = args.lr_fine\n",
    "        selection = 'gumbel-softmax'\n",
    "        temp = args.temp_init * math.exp(-args.eta * (ep-args.epochs_warmup))\n",
    "    return lr, selection, temp\n",
    "\n",
    "def train(model, optimizer, train_loader, selection, temp, logger, ep, p=0.5, weighted=False):\n",
    "    \n",
    "    mse_losses = []\n",
    "    penalties = []\n",
    "    data_len = len(train_loader)\n",
    "\n",
    "    for i, (imgs, imus, gts, rot, weight) in enumerate(train_loader):\n",
    "\n",
    "        imgs = imgs.cuda().float()\n",
    "        imus = imus.cuda().float()\n",
    "        gts = gts.cuda().float() \n",
    "        weight = weight.cuda().float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        poses, decisions, probs, _ = model(imgs, imus, is_first=True, hc=None, temp=temp, selection=selection, p=p)\n",
    "        \n",
    "        if not weighted:\n",
    "            angle_loss = torch.nn.functional.mse_loss(poses[:,:,:3], gts[:, :, :3])\n",
    "            translation_loss = torch.nn.functional.mse_loss(poses[:,:,3:], gts[:, :, 3:])\n",
    "        else:\n",
    "            weight = weight/weight.sum()\n",
    "            angle_loss = (weight.unsqueeze(-1).unsqueeze(-1) * (poses[:,:,:3] - gts[:, :, :3]) ** 2).mean()\n",
    "            translation_loss = (weight.unsqueeze(-1).unsqueeze(-1) * (poses[:,:,3:] - gts[:, :, 3:]) ** 2).mean()\n",
    "        \n",
    "        pose_loss = 100 * angle_loss + translation_loss        \n",
    "        penalty = (decisions[:,:,0].float()).sum(-1).mean()\n",
    "        loss = pose_loss + args.Lambda * penalty \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % args.print_frequency == 0: \n",
    "            message = f'Epoch: {ep}, iters: {i}/{data_len}, pose loss: {pose_loss.item():.6f}, penalty: {penalty.item():.6f}, loss: {loss.item():.6f}'\n",
    "            print(message)\n",
    "            logger.info(message)\n",
    "\n",
    "        mse_losses.append(pose_loss.item())\n",
    "        penalties.append(penalty.item())\n",
    "\n",
    "    return np.mean(mse_losses), np.mean(penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped scenes: 302\n",
      "total samples: 88624\n"
     ]
    }
   ],
   "source": [
    "mmcv.mkdir_or_exist(args.save_dir)\n",
    "checkpoints_dir = os.path.join(args.save_dir, \"experiment_1\")\n",
    "mmcv.mkdir_or_exist(checkpoints_dir)\n",
    "\n",
    "# Create logs\n",
    "logger = logging.getLogger(args.experiment_name)\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger.info('----------------------------------------TRAINING----------------------------------')\n",
    "logger.info('PARAMETER ...')\n",
    "logger.info(args)\n",
    "\n",
    "# Load the dataset\n",
    "transform_train = [custom_transform.ToTensor(), custom_transform.Resize((args.img_h, args.img_w))]\n",
    "if args.hflip:\n",
    "    transform_train += [custom_transform.RandomHorizontalFlip()]\n",
    "if args.color:\n",
    "    transform_train += [custom_transform.RandomColorAug()]\n",
    "transform_train = custom_transform.Compose(transform_train)\n",
    "\n",
    "##############################################################\n",
    "max_imu_length = 11 # KITTI\n",
    "##############################################################\n",
    "\n",
    "train_dataset = Nusc_Dataset(dataroot,\n",
    "                             sequence_length=args.seq_len,\n",
    "                             max_imu_length=max_imu_length,\n",
    "                             cam_names=cam_names,\n",
    "                             transform=transform_train,\n",
    "                             nusc=nusc,\n",
    "                             nusc_can=nusc_can)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True, # TODO false?\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU selections\n",
    "str_ids = device.split(',')\n",
    "gpu_ids = []\n",
    "for str_id in str_ids:\n",
    "    id = int(str_id)\n",
    "    if id >= 0:\n",
    "        gpu_ids.append(id)\n",
    "if len(gpu_ids) > 0:\n",
    "    torch.cuda.set_device(gpu_ids[0])\n",
    "\n",
    "# Model initialization\n",
    "model = DeepVIO(args)\n",
    "\n",
    "# Continual training or not\n",
    "if args.pretrain is not None:\n",
    "    model.load_state_dict(torch.load(args.pretrain))\n",
    "    print('load model %s'%args.pretrain)\n",
    "    logger.info('load model %s'%args.pretrain)\n",
    "else:\n",
    "    print('Training from scratch')\n",
    "    logger.info('Training from scratch')\n",
    "\n",
    "# Use the pre-trained flownet or not\n",
    "if args.pretrain_flownet and args.pretrain is None:\n",
    "    pretrained_w = torch.load(args.pretrain_flownet, map_location='cpu')\n",
    "    model_dict = model.Feature_net.state_dict()\n",
    "    update_dict = {k: v for k, v in pretrained_w['state_dict'].items() if k in model_dict}\n",
    "    model_dict.update(update_dict)\n",
    "    model.Feature_net.load_state_dict(model_dict)\n",
    "\n",
    "# Feed model to GPU\n",
    "# model.to(device)\n",
    "# model = torch.nn.DataParallel(model, device_ids = [device])\n",
    "\n",
    "# model = model.cuda()\n",
    "model.cuda(gpu_ids[0])\n",
    "model = torch.nn.DataParallel(model, device_ids = gpu_ids)\n",
    "\n",
    "pretrain = args.pretrain \n",
    "if args.pretrain is None or pretrain[-5:] == 'model':\n",
    "    init_epoch = 0\n",
    "else:\n",
    "    init_epoch = int(pretrain[-7:-4])+1\n",
    "\n",
    "# Initialize the optimizer\n",
    "if args.optimizer == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "elif args.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, weight_decay=args.weight_decay)\n",
    "\n",
    "best = 10000\n",
    "\n",
    "for ep in range(init_epoch, args.epochs_warmup+args.epochs_joint+args.epochs_fine):\n",
    "    lr, selection, temp = update_status(ep, args, model)\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    message = f'Epoch: {ep}, lr: {lr}, selection: {selection}, temperaure: {temp:.5f}'\n",
    "    print(message)\n",
    "    logger.info(message)\n",
    "    \n",
    "    model.train()\n",
    "    avg_pose_loss, avg_penalty_loss = train(model, optimizer, train_loader, selection, temp, logger, ep, p=0.5)\n",
    "    \n",
    "    # Save the model after training\n",
    "    # 어차피 args.epochs_warmup+args.epochs_joint 넘어가야 evaluation 및 best model 저장 진행하므로 애초에 그 이후부터 저장 \n",
    "    if ep > args.epochs_warmup+args.epochs_joint:\n",
    "        torch.save(model.module.state_dict(), f'{checkpoints_dir}/{ep:003}.pth')\n",
    "        message = f'Epoch {ep} training finished, pose loss: {avg_pose_loss:.6f}, penalty_loss: {avg_penalty_loss:.6f}, model saved'\n",
    "        print(message)\n",
    "        logger.info(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
